{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39eca615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: c:\\Users\\28623\\OneDrive\\Desktop\\BankJournalAnalysis\\notebooks\n",
      "项目根目录: c:\\Users\\28623\\OneDrive\\Desktop\\BankJournalAnalysis\n",
      "开始关键词频率分析...\n",
      " 分析期刊关键词频率...\n",
      "总论文数: 9747\n",
      "有关键词的论文: 7293\n",
      "总关键词数: 42534\n",
      "分析的期刊数: 10\n",
      "\n",
      " 期刊主题分析完成: 10 种期刊\n",
      "\n",
      " 期刊主题分布 (所有期刊):\n",
      "================================================================================\n",
      " 1. BIOMEDICAL ENGINEERING ONLINE:\n",
      "    1. model\n",
      "    2. system\n",
      "    3. classification\n",
      "    4. diagnosis\n",
      "    5. segmentation\n",
      "\n",
      " 2. IEEE PULSE:\n",
      "    1. pressure regulatory system\n",
      "    2. control parameters\n",
      "    3. cells\n",
      "    4. hepatocellular-carcinoma\n",
      "    5. humans\n",
      "\n",
      " 3. IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING:\n",
      "    1. model\n",
      "    2. system\n",
      "    3. classification\n",
      "    4. design\n",
      "    5. algorithm\n",
      "\n",
      " 4. NATURE BIOMEDICAL ENGINEERING:\n",
      "    1. expression\n",
      "    2. cancer\n",
      "    3. in-vivo\n",
      "    4. activation\n",
      "    5. delivery\n",
      "\n",
      " 5. MEDICAL ENGINEERING & PHYSICS:\n",
      "    1. model\n",
      "    2. system\n",
      "    3. mechanical-properties\n",
      "    4. pressure\n",
      "    5. validation\n",
      "\n",
      " 6. BIOMEDICAL SIGNAL PROCESSING AND CONTROL:\n",
      "    1. classification\n",
      "    2. diagnosis\n",
      "    3. algorithm\n",
      "    4. model\n",
      "    5. system\n",
      "\n",
      " 7. ANNALS OF BIOMEDICAL ENGINEERING:\n",
      "    1. model\n",
      "    2. in-vitro\n",
      "    3. mechanical-properties\n",
      "    4. flow\n",
      "    5. blood-flow\n",
      "\n",
      " 8. EXPERT REVIEW OF MEDICAL DEVICES:\n",
      "    1. management\n",
      "    2. outcomes\n",
      "    3. system\n",
      "    4. experience\n",
      "    5. efficacy\n",
      "\n",
      " 9. JOURNAL OF MEDICAL DEVICES-TRANSACTIONS OF THE ASME:\n",
      "    1. design\n",
      "    2. surgery\n",
      "    3. system\n",
      "    4. robot\n",
      "    5. performance\n",
      "\n",
      "10. JOURNAL OF MEDICAL AND BIOLOGICAL ENGINEERING:\n",
      "    1. system\n",
      "    2. in-vitro\n",
      "    3. model\n",
      "    4. diagnosis\n",
      "    5. design\n",
      "\n",
      "总共分析了 10 种期刊\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "import ast\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "import os\n",
    "# 获取当前工作目录（通常是 notebooks/）\n",
    "CURRENT_DIR = os.getcwd()\n",
    "print(\"当前工作目录:\", CURRENT_DIR)\n",
    "\n",
    "# 推断项目根目录：假设 notebooks 是子目录\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, '..'))\n",
    "\n",
    "print(\"项目根目录:\", PROJECT_ROOT)\n",
    "\n",
    "# 定义输出目录\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'disrupt')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 定义数据路径\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, 'top10_journals_data.csv'))\n",
    "\n",
    "# 定义关键词频率分析器\n",
    "class KeywordFrequencyAnalyzer:\n",
    "    \"\"\"关键词频率分析器 - 提取每个期刊的Top5关键词\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.results = {}\n",
    "    \n",
    "    def analyze(self, min_papers: int = 5, **kwargs) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        分析每个期刊的Top5关键词\n",
    "        \"\"\"\n",
    "        print(\" 分析期刊关键词频率...\")\n",
    "        \n",
    "        # 提取和清洗所有关键词\n",
    "        paper_keywords = self._extract_all_keywords()\n",
    "        \n",
    "        # 按期刊统计词频\n",
    "        journal_keyword_freq = self._calculate_journal_keyword_freq(paper_keywords, min_papers)\n",
    "        \n",
    "        # 保存词频数据供后续使用\n",
    "        self._journal_keyword_freq = journal_keyword_freq\n",
    "        \n",
    "        # 提取每个期刊的Top5关键词\n",
    "        journal_top_keywords = self._get_top_keywords(journal_keyword_freq)\n",
    "        \n",
    "        self.results = journal_top_keywords\n",
    "        return journal_top_keywords\n",
    "    \n",
    "    def _extract_all_keywords(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"提取所有论文的关键词\"\"\"\n",
    "        paper_keywords = {}\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            # 使用索引作为paper_id，避免doi空值问题\n",
    "            paper_id = f\"paper_{idx}\"\n",
    "            keywords = []\n",
    "            \n",
    "            if pd.notna(row.get('Keywords')):\n",
    "                try:\n",
    "                    keywords = self._parse_keyword_field(row['Keywords'])\n",
    "                except (ValueError, SyntaxError):\n",
    "                    keywords = []\n",
    "            \n",
    "            paper_keywords[paper_id] = keywords\n",
    "        \n",
    "        total_keywords = sum(len(keywords) for keywords in paper_keywords.values())\n",
    "        papers_with_keywords = sum(1 for keywords in paper_keywords.values() if keywords)\n",
    "        \n",
    "        print(f\"总论文数: {len(paper_keywords)}\")\n",
    "        print(f\"有关键词的论文: {papers_with_keywords}\")\n",
    "        print(f\"总关键词数: {total_keywords}\")\n",
    "        \n",
    "        return paper_keywords\n",
    "    \n",
    "    def _parse_keyword_field(self, word_field) -> List[str]:\n",
    "        \"\"\"解析word字段\"\"\"\n",
    "        if pd.isna(word_field):\n",
    "            return []\n",
    "        \n",
    "        if isinstance(word_field, str):\n",
    "            if word_field.startswith('['):\n",
    "                try:\n",
    "                    keywords = ast.literal_eval(word_field)\n",
    "                except:\n",
    "                    keywords = [word_field]\n",
    "            else:\n",
    "                if ';' in word_field:\n",
    "                    keywords = [kw.strip() for kw in word_field.split(';') if kw.strip()]\n",
    "                else:\n",
    "                    keywords = [kw.strip() for kw in word_field.split(',') if kw.strip()]\n",
    "        elif isinstance(word_field, list):\n",
    "            keywords = word_field\n",
    "        else:\n",
    "            keywords = [str(word_field)]\n",
    "        \n",
    "        cleaned_keywords = [str(kw).lower().strip() for kw in keywords if str(kw).strip()]\n",
    "        return cleaned_keywords\n",
    "    \n",
    "    def _calculate_journal_keyword_freq(self, paper_keywords: Dict[str, List[str]], min_papers: int) -> Dict[str, Counter]:\n",
    "        \"\"\"按期刊统计关键词频率\"\"\"\n",
    "        journal_keywords = {}\n",
    "        \n",
    "        for paper_id, keywords in paper_keywords.items():\n",
    "            # 直接从索引获取期刊信息\n",
    "            idx = int(paper_id.split('_')[1])\n",
    "            journal = self.df.iloc[idx]['Source Title']\n",
    "            \n",
    "            if pd.isna(journal):\n",
    "                continue\n",
    "            \n",
    "            if journal not in journal_keywords:\n",
    "                journal_keywords[journal] = []\n",
    "            \n",
    "            journal_keywords[journal].extend(keywords)\n",
    "        \n",
    "        # 过滤小期刊\n",
    "        journal_keyword_freq = {}\n",
    "        for journal, keywords in journal_keywords.items():\n",
    "            journal_papers = self.df[self.df['Source Title'] == journal]\n",
    "            if len(journal_papers) >= min_papers and keywords:\n",
    "                journal_keyword_freq[journal] = Counter(keywords)\n",
    "        \n",
    "        print(f\"分析的期刊数: {len(journal_keyword_freq)}\")\n",
    "        return journal_keyword_freq\n",
    "    \n",
    "    def _get_top_keywords(self, journal_keyword_freq: Dict[str, Counter]) -> Dict[str, List[str]]:\n",
    "        \"\"\"提取每个期刊的Top5关键词\"\"\"\n",
    "        journal_top_keywords = {}\n",
    "        for journal, keyword_counter in journal_keyword_freq.items():\n",
    "            top_keywords = keyword_counter.most_common(5)\n",
    "            journal_top_keywords[journal] = [keyword for keyword, count in top_keywords]\n",
    "        \n",
    "        return journal_top_keywords\n",
    "\n",
    "# 执行分析\n",
    "print(\"开始关键词频率分析...\")\n",
    "analyzer = KeywordFrequencyAnalyzer(df)\n",
    "journal_top_keywords = analyzer.analyze(min_papers=5)\n",
    "\n",
    "# 显示结果\n",
    "print(f\"\\n 期刊主题分析完成: {len(journal_top_keywords)} 种期刊\")\n",
    "# 显示所有期刊的Top5关键词\n",
    "print(\"\\n 期刊主题分布 (所有期刊):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (journal, keywords) in enumerate(sorted(journal_top_keywords.items(), key=lambda x: len(x[1]), reverse=True), 1):\n",
    "    print(f\"{i:2d}. {journal}:\")\n",
    "    for j, keyword in enumerate(keywords, 1):\n",
    "        print(f\"    {j}. {keyword}\")\n",
    "    print()  # 空行分隔\n",
    "\n",
    "print(f\"总共分析了 {len(journal_top_keywords)} 种期刊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始分析期刊主题...\n",
      "\n",
      "分析期刊: BIOMEDICAL ENGINEERING ONLINE\n",
      "该刊聚焦生物医学工程领域的数字化解决方案，以**计算建模**为核心，运用**机器学习分类算法**与**医学影像分割技术**，致力于构建智能化**诊断系统**。研究覆盖从器官级仿真到临床决策支持的多层次体系，侧重将工程技术转化为可落地的健康评估工具，应用于肿瘤检测、病理分级及个性化治疗规划等场景。✅ 分析完成\n",
      "\n",
      "分析期刊: IEEE PULSE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# 直接使用你的journal_top_keywords数据\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# 分析期刊主题\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     results = \u001b[43manalyze_journal_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjournal_top_keywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# 打印最终结果\u001b[39;00m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36manalyze_journal_topics\u001b[39m\u001b[34m(journal_top_keywords)\u001b[39m\n\u001b[32m     26\u001b[39m message = [\n\u001b[32m     27\u001b[39m     {\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt\n\u001b[32m     30\u001b[39m     }\n\u001b[32m     31\u001b[39m ]\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     analysis = \u001b[43mget_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     results[journal] = analysis\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ 分析完成\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mget_answer\u001b[39m\u001b[34m(message)\u001b[39m\n\u001b[32m     59\u001b[39m full_response = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     61\u001b[39m response = requests.post(url=url, json=body, headers=headers, stream=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[DONE]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# 处理数据格式\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\28623\\OneDrive\\Desktop\\BankJournalAnalysis\\.venv\\Lib\\site-packages\\requests\\models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\28623\\OneDrive\\Desktop\\BankJournalAnalysis\\.venv\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\28623\\OneDrive\\Desktop\\BankJournalAnalysis\\.venv\\Lib\\site-packages\\urllib3\\response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\28623\\OneDrive\\Desktop\\BankJournalAnalysis\\.venv\\Lib\\site-packages\\urllib3\\response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\28623\\OneDrive\\Desktop\\BankJournalAnalysis\\.venv\\Lib\\site-packages\\urllib3\\response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# 请替换为你的 API Key\n",
    "api_key = \"Bearer cyjdtVYXSGWgwiUdnLMs:DvKIMQbkHgKlYljNcbhN\"  # 你的实际API Key\n",
    "url = \"https://spark-api-open.xf-yun.com/v2/chat/completions\"\n",
    "\n",
    "def analyze_journal_topics(journal_top_keywords):\n",
    "    \"\"\"分析期刊主题\"\"\"\n",
    "    print(\"开始分析期刊主题...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for journal, keywords in journal_top_keywords.items():\n",
    "        print(f\"\\n分析期刊: {journal}\")\n",
    "        \n",
    "        # 构建提示词\n",
    "        prompt = f\"\"\"请根据这个学术期刊的名称和其高频关键词，简要分析该期刊的主要研究方向和主题侧重点。\n",
    "\n",
    "期刊名称：{journal}\n",
    "高频关键词：{', '.join(keywords)}\n",
    "\n",
    "请用100字左右描述该期刊的研究主题特点、技术方法和应用领域。直接输出分析结果，不要有任何思维过程。\"\"\"\n",
    "        \n",
    "        # 构建消息格式\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            analysis = get_answer(message)\n",
    "            results[journal] = analysis\n",
    "            print(f\"✅ 分析完成\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 分析失败: {e}\")\n",
    "            results[journal] = \"分析失败\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_answer(message):\n",
    "    \"\"\"调用星火API获取回答\"\"\"\n",
    "    headers = {\n",
    "        'Authorization': api_key,\n",
    "        'content-type': \"application/json\"\n",
    "    }\n",
    "    body = {\n",
    "        \"model\": \"x1\",\n",
    "        \"user\": \"journal_analyzer\",\n",
    "        \"messages\": message,\n",
    "        \"stream\": True,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.7,\n",
    "        \"reasoning\": False  # 关键：完全禁用思维链\n",
    "    }\n",
    "    \n",
    "    full_response = \"\"\n",
    "\n",
    "    response = requests.post(url=url, json=body, headers=headers, stream=True)\n",
    "    \n",
    "    for chunks in response.iter_lines():\n",
    "        if chunks and b'[DONE]' not in chunks:\n",
    "            try:\n",
    "                # 处理数据格式\n",
    "                if chunks.startswith(b'data: '):\n",
    "                    data_org = chunks[6:]\n",
    "                else:\n",
    "                    data_org = chunks\n",
    "                \n",
    "                chunk = json.loads(data_org)\n",
    "                text = chunk['choices'][0]['delta']\n",
    "                \n",
    "                # 只收集最终回复\n",
    "                if 'content' in text and text['content']:\n",
    "                    content = text[\"content\"]\n",
    "                    print(content, end=\"\")\n",
    "                    full_response += content\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# 直接使用你的journal_top_keywords数据\n",
    "if __name__ == '__main__':\n",
    "    # 分析期刊主题\n",
    "    results = analyze_journal_topics(journal_top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bf0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始分析期刊主题...\n",
      "\n",
      "分析期刊: BIOMEDICAL ENGINEERING ONLINE\n",
      "该期刊聚焦生物医学工程领域的计算建模与智能系统开发，以机器学习驱动的疾病诊断为核心，重点研究医学影像自动分割、生物信号分类算法及临床决策支持系统。技术方法涵盖深度学习模型构建、多模态数据融合与模式识别，应用于肿瘤检测、神经疾病分析和手术导航等精准医疗场景，强调算法创新与临床转化的结合。✅ 分析完成\n",
      "\n",
      "分析期刊: IEEE PULSE\n",
      "IEEE PULSE聚焦生物医学工程与临床医学交叉领域，主要研究压力调控系统在生理功能优化及疾病治疗中的应用。其核心方向包括闭环控制系统设计、生物参数动态监测技术，尤其关注肝细胞癌微环境压力调控机制及抗癌疗法开发。技术手段涵盖生物信号传感、非线性控制算法建模及细胞级病理模型构建，应用场景集中于智能医疗设备研发、精准肿瘤治疗及人体生理机能干预，体现从分子机制探索到临床转化的创新路径。✅ 分析完成\n",
      "\n",
      "分析期刊: IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING\n",
      "该期刊聚焦生物医学工程领域的技术创新，以建模、系统设计、分类方法和算法开发为核心，研究智能医疗设备、生理信号处理及健康监测系统。侧重运用机器学习、数据分析和工程优化手段解决临床医学、康复工程及生物信息学中的技术难题，推动精准诊疗与个性化医疗发展。✅ 分析完成\n",
      "\n",
      "分析期刊: NATURE BIOMEDICAL ENGINEERING\n",
      "该期刊聚焦于生物医学工程在癌症研究中的创新应用，以基因/蛋白**表达**调控为核心，结合**体内**（in-vivo）实验验证，探索靶向**癌症**治疗的**递送**系统设计与**激活**机制。主要采用纳米技术、基因编辑及智能材料等手段，开发精准诊疗技术，侧重从分子层面实现药物可控释放与肿瘤微环境响应，推动个性化抗癌疗法转化。✅ 分析完成\n",
      "\n",
      "分析期刊: MEDICAL ENGINEERING & PHYSICS\n",
      "该期刊聚焦医学工程与物理原理的交叉应用，以建模（model）、系统（system）为核心方法，侧重生物材料机械性能（mechanical-properties）、压力（pressure）参数分析及技术验证（validation）。主要研究涵盖医疗设备创新设计、生物力学仿真、诊疗系统开发，涉及流体动力学、组织力学等领域，强调通过数值模拟与实验验证相结合，推动医疗器械研发、康复工程及临床诊疗技术的精准化发展。✅ 分析完成\n",
      "\n",
      "分析期刊: BIOMEDICAL SIGNAL PROCESSING AND CONTROL\n",
      "该期刊聚焦生物医学信号处理与智能控制，核心方向为基于算法创新的信号分类、疾病诊断模型构建及闭环控制系统开发。运用机器学习、数学建模与工程化系统设计，整合多模态生理数据，推动精准医疗、可穿戴设备及远程监护领域的技术转化，强调算法实时性与临床实用性平衡。✅ 分析完成\n",
      "\n",
      "分析期刊: ANNALS OF BIOMEDICAL ENGINEERING\n",
      "该刊聚焦生物医学工程交叉领域，以**多尺度建模仿真**为核心方法论，重点探究人体器官/组织的**力学特性表征**及**血流动力学机制**，通过构建精密化的**体外仿生系统**解析复杂生理过程。技术路径融合连续介质力学、计算流体力学与微纳传感测量，广泛应用于心血管器械研发、肿瘤微环境模拟、智能药物递送系统等场景，凸显“医工结合”的实践导向特征。✅ 分析完成\n",
      "\n",
      "分析期刊: EXPERT REVIEW OF MEDICAL DEVICES\n",
      "该期刊聚焦医疗设备领域的临床实践与技术创新，侧重研究设备全生命周期管理、临床疗效评估及系统集成优化。通过循证医学方法验证设备效能，结合多中心临床数据与专家经验，探讨新型诊疗器械的安全性、操作规范及卫生经济学评价。主要覆盖心血管介入装置、智能监测系统、微创手术机器人等高端医疗器械，强调跨学科协作下的技术转化与医疗质量提升。✅ 分析完成\n",
      "\n",
      "分析期刊: JOURNAL OF MEDICAL DEVICES-TRANSACTIONS OF THE ASME\n",
      "该刊聚焦医疗器械研发，侧重手术机器人、智能诊疗系统的设计与性能优化，融合生物医学工程与先进制造技术。通过仿真测试、临床验证等手段提升设备安全性和操作精度，主要应用于微创外科、康复治疗等领域，强调技术创新与临床转化的结合。✅ 分析完成\n",
      "\n",
      "分析期刊: JOURNAL OF MEDICAL AND BIOLOGICAL ENGINEERING\n",
      "该期刊聚焦于医学与生物工程交叉领域，以**系统设计**为核心，侧重**体外（in-vitro）模型**开发及**疾病诊断**技术创新。研究涵盖生物医学装置的工程设计、生理系统仿真建模、新型诊疗技术转化等方向，强调通过跨学科方法解决临床问题，应用于精准医疗、药物筛选及再生医学等领域，体现“设计-验证-应用”全链条研究特色。✅ 分析完成\n",
      "✅ 期刊关键词表已保存至:\n",
      "   c:\\Users\\28623\\OneDrive\\Desktop\\BankJournalAnalysis\\outputs\\theme\\journal_keywords.json\n",
      "✅ 期刊主题分析结果已保存至:\n",
      "   c:\\Users\\28623\\OneDrive\\Desktop\\BankJournalAnalysis\\outputs\\theme\\journal_analysis.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 定义输出目录：项目根目录下的 outputs/theme\n",
    "OUTPUT_THEME_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'theme')\n",
    "os.makedirs(OUTPUT_THEME_DIR, exist_ok=True)\n",
    "\n",
    "# 输出文件路径\n",
    "keywords_output_path = os.path.join(OUTPUT_THEME_DIR, 'journal_keywords.json')\n",
    "analysis_output_path = os.path.join(OUTPUT_THEME_DIR, 'journal_analysis.json')\n",
    "\n",
    "# 准备元数据\n",
    "metadata = {\n",
    "    \"source_file\": \"top10_journals_data.csv\",\n",
    "    \"total_journals\": len(journal_top_keywords)\n",
    "}\n",
    "\n",
    "# === 输出 1: Top5 关键词表 ===\n",
    "keywords_data = {\n",
    "    \"metadata\": metadata,\n",
    "    \"data\": journal_top_keywords  # {期刊: [Top5关键词]}\n",
    "}\n",
    "\n",
    "with open(keywords_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(keywords_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ 期刊关键词表已保存至:\\n   {keywords_output_path}\")\n",
    "\n",
    "# === 输出 2: AI 主题分析结果 ===\n",
    "analysis_data = {\n",
    "    \"metadata\": metadata,\n",
    "    \"data\": results  # {期刊: \"AI分析文本\"}\n",
    "}\n",
    "\n",
    "with open(analysis_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ 期刊主题分析结果已保存至:\\n   {analysis_output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
