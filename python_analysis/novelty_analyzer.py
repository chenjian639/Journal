# -*- coding: utf-8 -*-
"""
python_analysis/novelty_analyzer.py
æœŸåˆŠæ–°é¢–æ€§æŒ‡æ•°åˆ†æç³»ç»Ÿï¼ˆåŸºäº Uzzi et al., Science 2013 çš„ç»„åˆæ–°é¢–æ€§æ–¹æ³•ï¼‰
"""

# === åŸºç¡€åº“ ===
import os
import json
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
from itertools import combinations
import ast

# === å¯è§†åŒ–æ”¯æŒ ===
import matplotlib.pyplot as plt
import seaborn as sns

# === è®¾ç½® ===
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
pd.options.mode.chained_assignment = None


class NoveltyAnalyzer:
    """
    ç»„åˆæ–°é¢–æ€§åˆ†æå™¨ï¼ˆCombination Noveltyï¼‰
    åŸºäºå…³é”®è¯å…±ç°æ¨¡å¼è¯†åˆ«â€œå‰æ‰€æœªæœ‰â€çš„çŸ¥è¯†ç»„åˆ
    """

    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
        self.paper_keywords = {}           # DOI -> [keywords]
        self.keyword_pairs_first_seen = {} # (k1,k2) -> first_year
        self.results = {}

    def analyze(self) -> dict:
        """
        æ‰§è¡Œç»„åˆæ–°é¢–æ€§åˆ†æ
        
        æµç¨‹ï¼š
        1. æå–æ¯ç¯‡è®ºæ–‡çš„å…³é”®è¯
        2. æ„å»ºæ‰€æœ‰å…³é”®è¯å¯¹çš„å†å²é¦–æ¬¡å‡ºç°å¹´ä»½
        3. è®¡ç®—æ¯ç¯‡è®ºæ–‡ä¸­â€œé¦–æ¬¡å‡ºç°â€çš„æ–°ç»„åˆæ¯”ä¾‹
        4. æŒ‰æœŸåˆŠèšåˆä¸ºå¹³å‡æ–°é¢–æ€§å¾—åˆ†
        
        Returns:
            {æœŸåˆŠå: æ–°é¢–æ€§å¾—åˆ†}
        """
        print("ğŸ” å¼€å§‹ç»„åˆæ–°é¢–æ€§åˆ†æï¼ˆUzzi æ–¹æ³•ï¼‰...")

        # Step 1: æå–å…³é”®è¯
        self._extract_paper_keywords()

        # Step 2: æ„å»ºå…¨å±€å…³é”®è¯å¯¹æ—¶é—´çº¿
        self._build_keyword_pair_timeline()

        # Step 3: è®¡ç®—æ¯ç¯‡è®ºæ–‡çš„æ–°é¢–æ€§å¾—åˆ†
        paper_novelty_df = self._calculate_paper_combination_novelty()

        # Step 4: æŒ‰æœŸåˆŠè®¡ç®—å¹³å‡å¾—åˆ†
        journal_novelty = self._aggregate_to_journal(paper_novelty_df)

        self.results = journal_novelty
        return journal_novelty

    def _extract_paper_keywords(self):
        """æå–å¹¶æ¸…æ´—å…³é”®è¯"""
        print("ğŸ“ æå–è®ºæ–‡å…³é”®è¯...")
        stop_words = {
            'review', 'studies', 'study', 'analysis', 'method', 'methods',
            'approach', 'approaches', 'framework', 'model', 'system',
            'based', 'using', 'via', 'case study', 'research', 'development'
        }

        for _, row in self.df.iterrows():
            paper_id = row['DOI'] if pd.notna(row['DOI']) else f"paper_{_}"
            keywords = []

            if pd.notna(row.get('Keywords')):
                word_content = str(row['Keywords']).strip()
                try:
                    if word_content.startswith('[') and word_content.endswith(']'):
                        raw_list = ast.literal_eval(word_content)
                        keywords = [str(kw).strip().lower() for kw in raw_list if str(kw).strip()]
                    else:
                        keywords = [kw.strip().lower() for kw in word_content.replace(';', ',').split(',') if kw.strip()]
                except (ValueError, SyntaxError):
                    keywords = [word_content.lower()] if word_content else []

            # æ¸…æ´—ï¼šå»åœç”¨è¯ã€å»ç©ºæ ¼ã€æ ‡å‡†åŒ–
            keywords = [kw for kw in keywords if kw not in stop_words and len(kw) > 1]
            self.paper_keywords[paper_id] = sorted(set(keywords))  # æ’åºä¾¿äº pair ä¸€è‡´

        total_papers = len(self.paper_keywords)
        papers_with_kw = sum(1 for kw in self.paper_keywords.values() if kw)
        total_keywords = sum(len(kw) for kw in self.paper_keywords.values())
        unique_keywords = len(set(kw for kws in self.paper_keywords.values() for kw in kws))

        print(f"âœ… å…³é”®è¯æå–å®Œæˆ: {papers_with_kw}/{total_papers} ç¯‡æœ‰å…³é”®è¯")
        print(f"   å…± {unique_keywords} ä¸ªç‹¬ç‰¹å…³é”®è¯, æ€»å…±å‡ºç° {total_keywords} æ¬¡")

    def _build_keyword_pair_timeline(self):
        """æ„å»ºæ‰€æœ‰å…³é”®è¯å¯¹çš„é¦–æ¬¡å‡ºç°å¹´ä»½"""
        print("ğŸ“… æ„å»ºå…³é”®è¯ç»„åˆæ—¶é—´çº¿...")
        pair_first_year = defaultdict(lambda: float('inf'))

        missing_year_count = 0
        for paper_id, keywords in self.paper_keywords.items():
            if len(keywords) < 2:
                continue

            year = self._get_paper_year(paper_id)
            if not year:
                missing_year_count += 1
                continue

            # ç”Ÿæˆæ‰€æœ‰æ— åºä¸¤ä¸¤ç»„åˆ
            for pair in combinations(keywords, 2):
                # è§„èŒƒåŒ–é¡ºåºï¼šå­—å…¸åºï¼Œç¡®ä¿ ('a','b') == ('b','a')
                norm_pair = tuple(sorted(pair))
                if year < pair_first_year[norm_pair]:
                    pair_first_year[norm_pair] = year

        self.keyword_pairs_first_seen = dict(pair_first_year)
        print(f"âœ… æ„å»ºå®Œæˆ: å…± {len(pair_first_year)} ä¸ªå…³é”®è¯å¯¹ç»„åˆ")
        if missing_year_count:
            print(f"âš ï¸  {missing_year_count} ç¯‡è®ºæ–‡ç¼ºå°‘å¹´ä»½ä¿¡æ¯è¢«è·³è¿‡")

    def _get_paper_year(self, paper_id: str) -> int:
        """è·å–è®ºæ–‡å‡ºç‰ˆå¹´ä»½"""
        try:
            if 'DOI' not in self.df.columns:
                return None
            matched = self.df[self.df['DOI'] == paper_id]
            if len(matched) == 0:
                return None
            year_val = matched.iloc[0]['Publication Year']
            return int(year_val) if pd.notna(year_val) else None
        except Exception:
            return None

    def _calculate_paper_combination_novelty(self) -> pd.DataFrame:
        """è®¡ç®—æ¯ç¯‡è®ºæ–‡çš„ç»„åˆæ–°é¢–æ€§å¾—åˆ†"""
        print("ğŸ¯ è®¡ç®—è®ºæ–‡ç»„åˆæ–°é¢–æ€§å¾—åˆ†...")
        results = []

        current_year = self.df['Publication Year'].max() if 'Publication Year' in self.df.columns else 2024

        for paper_id, keywords in self.paper_keywords.items():
            if len(keywords) < 2:
                results.append({'DOI': paper_id, 'novelty_score': np.nan})
                continue

            pairs = list(combinations(keywords, 2))
            novel_pairs = 0
            total_pairs = len(pairs)

            for pair in pairs:
                norm_pair = tuple(sorted(pair))
                first_year = self.keyword_pairs_first_seen.get(norm_pair, None)

                if first_year is None:
                    # å®Œå…¨æœªè§çš„ç»„åˆ â†’ ææ–°é¢–
                    novel_pairs += 1
                elif abs(first_year - current_year) <= 1:  # å½“å‰æˆ–å‰ä¸€å¹´æ‰é¦–æ¬¡å‡ºç°
                    novel_pairs += 1

            # æ–°é¢–æ€§å¾—åˆ† = æ–°ç»„åˆå æ¯”
            novelty_score = novel_pairs / total_pairs if total_pairs > 0 else np.nan
            results.append({'DOI': paper_id, 'novelty_score': novelty_score})

        df_out = pd.DataFrame(results)
        valid_count = df_out['novelty_score'].notna().sum()
        print(f"âœ… å®Œæˆ: {valid_count}/{len(df_out)} ç¯‡è®ºæ–‡è·å¾—æœ‰æ•ˆå¾—åˆ†")
        return df_out

    def _aggregate_to_journal(self, paper_novelty_df: pd.DataFrame) -> dict:
        """æŒ‰æœŸåˆŠèšåˆå¹³å‡æ–°é¢–æ€§å¾—åˆ†"""
        print("ğŸ“š æŒ‰æœŸåˆŠèšåˆç»“æœ...")

        # åˆå¹¶æœŸåˆŠä¿¡æ¯
        journal_map = self.df.drop_duplicates('DOI')[['DOI', 'Source Title']].set_index('DOI')['Source Title'].to_dict()
        paper_novelty_df['Source Title'] = paper_novelty_df['DOI'].map(journal_map)

        # è¿‡æ»¤æ— æ•ˆå€¼
        valid_df = paper_novelty_df.dropna(subset=['novelty_score', 'Source Title'])

        # è®¡ç®—æ¯ç§æœŸåˆŠçš„å¹³å‡æ–°é¢–æ€§
        journal_scores = valid_df.groupby('Source Title')['novelty_score'].mean().round(6).to_dict()

        print(f"âœ… èšåˆå®Œæˆ: å…± {len(journal_scores)} ç§æœŸåˆŠ")
        return journal_scores

    def get_detailed_results(self) -> dict:
        """è¿”å›è¯¦ç»†ä¸­é—´ç»“æœï¼ˆç”¨äºè°ƒè¯•æˆ–æ‰©å±•ï¼‰"""
        return {
            'journal_novelty': self.results,
            'paper_novelty': self._calculate_paper_combination_novelty(),
            'keyword_pairs_first_seen': self.keyword_pairs_first_seen,
            'paper_keywords': self.paper_keywords
        }


def run_novelty_analysis(
    background_data_path: str = '../data/raw/data_with_citing.csv',
    target_data_path: str = '../data/raw/top10_journals_data.csv',
    output_dir: str = '../outputs/novelty'
) -> dict:
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼šä½¿ç”¨å…¨é‡æ•°æ®æ„å»ºèƒŒæ™¯çŸ¥è¯†åº“ï¼Œè¯„ä¼° Top10 æœŸåˆŠçš„æ–°é¢–æ€§
    """
    # ========== 1. æ¨å¯¼é¡¹ç›®æ ¹ç›®å½• ==========
    try:
        current_dir = os.path.dirname(__file__)
    except NameError:
        current_dir = os.getcwd()
    project_root = os.path.abspath(os.path.join(current_dir, '..'))

    bg_path = os.path.join(project_root, background_data_path.lstrip('./'))
    tg_path = os.path.join(project_root, target_data_path.lstrip('./'))
    out_dir = os.path.join(project_root, output_dir.lstrip('./'))

    os.makedirs(out_dir, exist_ok=True)

    print(f"ğŸ“ åŠ è½½èƒŒæ™¯æ•°æ®ï¼ˆå…¨é‡ï¼‰: {bg_path}")
    df_background = pd.read_csv(bg_path)

    print(f"ğŸ“ åŠ è½½ç›®æ ‡æ•°æ®ï¼ˆTop10ï¼‰: {tg_path}")
    df_target = pd.read_csv(tg_path)

    # ========== 2. ä½¿ç”¨å…¨é‡æ•°æ®æ„å»ºç»„åˆæ—¶é—´çº¿ ==========
    print("\nğŸ”„ æ­£åœ¨ä½¿ç”¨å…¨é‡æ•°æ®æ„å»ºå…³é”®è¯ç»„åˆæ—¶é—´çº¿...")
    analyzer_bg = NoveltyAnalyzer(df_background)
    
    # æˆ‘ä»¬åªéœ€è¦å®ƒçš„ _extract å’Œ _build åŠŸèƒ½
    analyzer_bg._extract_paper_keywords()
    analyzer_bg._build_keyword_pair_timeline()  # â† å…³é”®ï¼šå…¨å±€ç»„åˆæ•°æ®åº“

    print(f"âœ… å…¨å±€æ—¶é—´çº¿æ„å»ºå®Œæˆ | å…± {len(analyzer_bg.keyword_pairs_first_seen)} ä¸ªå…³é”®è¯å¯¹")

    # ========== 3. åœ¨ Top10 æ•°æ®ä¸Šè®¡ç®—æ–°é¢–æ€§ï¼ˆä½¿ç”¨å…¨å±€æ—¶é—´çº¿ï¼‰==========
    print("\nğŸ“Š å¼€å§‹è®¡ç®— Top10 æœŸåˆŠçš„ç»„åˆæ–°é¢–æ€§...")
    analyzer_target = NoveltyAnalyzer(df_target)
    analyzer_target._extract_paper_keywords()

    # æ³¨å…¥å…¨å±€ç»„åˆæ—¶é—´çº¿ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼ï¼‰
    analyzer_target.keyword_pairs_first_seen = analyzer_bg.keyword_pairs_first_seen

    # æ­£å¸¸è®¡ç®—å¾—åˆ†ï¼ˆç°åœ¨æ˜¯åŸºäºå…¨å±€èƒŒæ™¯ï¼‰
    paper_novelty_df = analyzer_target._calculate_paper_combination_novelty()
    journal_novelty = analyzer_target._aggregate_to_journal(paper_novelty_df)

    analyzer_target.results = journal_novelty

    # ========== 4. è¾“å‡ºç»“æœ ==========
    result_json = os.path.join(out_dir, 'journal_novelty_scores.json')
    with open(result_json, 'w', encoding='utf-8') as f:
        json.dump(journal_novelty, f, ensure_ascii=False, indent=2)
    print(f"âœ… å·²ä¿å­˜ JSON: {result_json}")

    result_df = pd.DataFrame(list(journal_novelty.items()), 
                           columns=['Source Title', 'novelty_score'])
    result_df = result_df.sort_values('novelty_score', ascending=False).reset_index(drop=True)

    result_csv = os.path.join(out_dir, 'journal_novelty_ranking.csv')
    result_df.to_csv(result_csv, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²ä¿å­˜ CSV: {result_csv}")

    txt_path = os.path.join(out_dir, 'top_journals_by_novelty.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        for journal in result_df['Source Title']:
            f.write(f"{journal}\n")
    print(f"âœ… å·²ä¿å­˜ TXT: {txt_path}")

    # ========== 5. å¯è§†åŒ– ==========
    try:
        top10 = result_df.head(10)
        plt.figure(figsize=(12, 8))
        bars = plt.barh(top10['Source Title'], top10['novelty_score'], color='steelblue', alpha=0.8)

        for bar, val in zip(bars, top10['novelty_score']):
            plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
                     f'{val:.4f}', ha='left', va='center', fontsize=10,
                     bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))

        plt.xlabel('ç»„åˆæ–°é¢–æ€§å¾—åˆ†ï¼ˆåŸºäºå…¨é‡èƒŒæ™¯ï¼‰')
        plt.title('æœŸåˆŠç»„åˆæ–°é¢–æ€§æ’åï¼ˆå‰10ï¼‰\n(Uzzi et al., Science 2013) - å…¨å±€çŸ¥è¯†åŸºçº¿')
        plt.gca().invert_yaxis()
        plt.tight_layout()

        img_path = os.path.join(out_dir, 'novelty_ranking.png')
        plt.savefig(img_path, dpi=150, bbox_inches='tight')
        plt.show()
        print(f"âœ… å·²ä¿å­˜å›¾è¡¨: {img_path}")
    except Exception as e:
        print(f"âš ï¸ å›¾è¡¨ä¿å­˜å¤±è´¥: {e}")

    return {
        'analyzer': analyzer_target,
        'results': journal_novelty,
        'ranking': result_df,
        'global_analyzer': analyzer_bg
    }

# ========================
# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œåˆ™æ‰§è¡Œä¸»æµç¨‹
# ========================
if __name__ == "__main__":
    run_novelty_analysis()
