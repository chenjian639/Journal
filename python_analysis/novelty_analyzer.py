# -*- coding: utf-8 -*-
"""
python_analysis/novelty_analyzer.py
æœŸåˆŠæ–°é¢–æ€§æŒ‡æ•°åˆ†æç³»ç»Ÿ - ä¿®æ­£ç‰ˆ
ç™¾åˆ†åˆ¶å¾—åˆ† = æ–°é¢–æ€§å¾—åˆ† Ã— 600
è¾“å‡ºï¼šæ–°é¢–æ€§å¾—åˆ†åˆ—è¡¨ + ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾
"""
import json
import pandas as pd
import numpy as np
import ast
import matplotlib.pyplot as plt
from pathlib import Path
from collections import defaultdict, Counter
from itertools import combinations

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib.font_manager as fm
try:
    font_path = "C:/Windows/Fonts/simhei.ttf"
    if Path(font_path).exists():
        fm.fontManager.addfont(font_path)
        font_name = fm.FontProperties(fname=font_path).get_name()
        plt.rcParams['font.sans-serif'] = [font_name]
    plt.rcParams['axes.unicode_minus'] = False
except:
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).resolve().parent.parent / 'config.json'
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    return config.get("novelty", {})

def log(msg):
    print(f"[novelty] {msg}")

def clean_keywords(keywords_str):
    """æ¸…æ´—å…³é”®è¯"""
    if pd.isna(keywords_str):
        return []
    
    if isinstance(keywords_str, str):
        if keywords_str.startswith('[') and keywords_str.endswith(']'):
            try:
                return ast.literal_eval(keywords_str)
            except:
                pass
        
        # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²
        separators = [';', ',', '|', 'ã€']
        for sep in separators:
            if sep in keywords_str:
                return [k.strip().lower() for k in keywords_str.split(sep) if k.strip()]
        
        return [keywords_str.strip().lower()]
    
    return []

def calculate_percent_score(novelty_score):
    """è®¡ç®—ç™¾åˆ†åˆ¶å¾—åˆ†ï¼šæ–°é¢–æ€§å¾—åˆ† Ã— 600"""
    if pd.isna(novelty_score):
        return 0.0
    percent_score = novelty_score * 600
    return percent_score  # ä¿ç•™åŸå§‹å°æ•°ä½æ•°

class NoveltyAnalyzer:
    def __init__(self, config=None):
        self.config = config or load_config()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config['output']['novelty_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir = output_dir

    def run_analysis(self):
        """è¿è¡Œæ–°é¢–æ€§åˆ†æ"""
        try:
            log("=" * 50)
            log("å¼€å§‹æœŸåˆŠæ–°é¢–æ€§åˆ†æï¼ˆUzziç»„åˆæ–¹æ³•ï¼‰")
            log("=" * 50)
            
            # åŠ è½½æ•°æ®
            project_root = Path(__file__).resolve().parent.parent
            data_config = self.config.get('data_sources', {})
            
            # åŠ è½½èƒŒæ™¯æ•°æ®ï¼ˆæ„å»ºæ—¶é—´çº¿ï¼‰
            bg_path = project_root / data_config['all_data']
            tg_path = project_root / data_config['target_data']
            
            log(f"ğŸ“‚ åŠ è½½æ•°æ®...")
            log(f"  èƒŒæ™¯æ•°æ®: {bg_path}")
            log(f"  ç›®æ ‡æ•°æ®: {tg_path}")
            
            background_df = pd.read_csv(bg_path)
            target_df = pd.read_csv(tg_path)
            
            # è·å–åˆ—å
            id_col = self.config['columns']['id']
            journal_col = self.config['columns']['journal']
            keywords_col = self.config['columns']['keywords']
            year_col = self.config['columns']['year']
            
            log(f"ä½¿ç”¨åˆ—: ID={id_col}, æœŸåˆŠ={journal_col}, å…³é”®è¯={keywords_col}, å¹´ä»½={year_col}")
            
            # é˜¶æ®µ1: ä½¿ç”¨èƒŒæ™¯æ•°æ®æ„å»ºå…³é”®è¯å¯¹æ—¶é—´çº¿
            log("\nğŸ“Š æ„å»ºå…³é”®è¯å¯¹æ—¶é—´çº¿ï¼ˆèƒŒæ™¯æ•°æ®ï¼‰...")
            bg_pair_timeline = self._build_pair_timeline(background_df, id_col, keywords_col, year_col)
            
            # é˜¶æ®µ2: è®¡ç®—ç›®æ ‡æ•°æ®çš„æ–°é¢–æ€§
            log("ğŸ¯ è®¡ç®—ç›®æ ‡æœŸåˆŠæ–°é¢–æ€§...")
            journal_scores = self._calculate_target_novelty(target_df, bg_pair_timeline, 
                                                          id_col, journal_col, keywords_col, year_col)
            
            # é˜¶æ®µ3: è®¡ç®—ç™¾åˆ†åˆ¶å¾—åˆ†
            log("ğŸ“ˆ è®¡ç®—ç™¾åˆ†åˆ¶å¾—åˆ†...")
            for journal in journal_scores:
                if 'novelty_score' in journal_scores[journal]:
                    raw_score = journal_scores[journal]['novelty_score']
                    journal_scores[journal]['percent_score'] = calculate_percent_score(raw_score)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
            self.generate_outputs(journal_scores)
            
            log("\nâœ… åˆ†æå®Œæˆï¼")
            log(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
            log(f"  1. journal_novelty_scores.csv - æœŸåˆŠæ–°é¢–æ€§å¾—åˆ†åˆ—è¡¨")
            log(f"  2. journal_novelty_percent_chart.png - ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾")
            
            return journal_scores
            
        except Exception as e:
            log(f"é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    def _build_pair_timeline(self, df, id_col, keywords_col, year_col):
        """æ„å»ºå…³é”®è¯å¯¹é¦–æ¬¡å‡ºç°æ—¶é—´çº¿"""
        pair_year = defaultdict(lambda: float('inf'))
        
        for idx, row in df.iterrows():
            paper_id = str(row[id_col]) if pd.notna(row.get(id_col)) else f"paper_{idx}"
            
            # è·å–å…³é”®è¯
            keywords = []
            if keywords_col in row and pd.notna(row[keywords_col]):
                keywords = clean_keywords(row[keywords_col])
            
            if len(keywords) < 2:
                continue
            
            # è·å–å¹´ä»½
            if year_col in row and pd.notna(row[year_col]):
                try:
                    year = int(float(row[year_col]))
                except:
                    continue
            else:
                continue
            
            # ç”Ÿæˆæ‰€æœ‰å…³é”®è¯å¯¹
            for pair in combinations(keywords, 2):
                norm_pair = tuple(sorted(pair))
                if year < pair_year[norm_pair]:
                    pair_year[norm_pair] = year
        
        log(f"æ—¶é—´çº¿æ„å»ºå®Œæˆ: {len(pair_year)} ä¸ªå…³é”®è¯å¯¹")
        return dict(pair_year)

    def _calculate_target_novelty(self, df, pair_timeline, id_col, journal_col, keywords_col, year_col):
        """è®¡ç®—ç›®æ ‡æ•°æ®çš„æ–°é¢–æ€§"""
        # ä»é…ç½®è·å–å‚æ•°
        threshold_years = self.config.get('parameters', {}).get('novel_threshold_years', 1)
        
        # ç¡®å®šå½“å‰å¹´ä»½ï¼ˆä½¿ç”¨ç›®æ ‡æ•°æ®çš„æœ€æ–°å¹´ä»½ï¼‰
        current_year = df[year_col].max() if year_col in df.columns else pd.Timestamp.now().year
        if pd.isna(current_year):
            current_year = pd.Timestamp.now().year
        else:
            current_year = int(current_year)
        
        log(f"æ–°é¢–æ€§å‚æ•°: é˜ˆå€¼={threshold_years}å¹´, åŸºå‡†å¹´ä»½={current_year}")
        
        # æŒ‰è®ºæ–‡è®¡ç®—æ–°é¢–æ€§
        paper_results = []
        journal_results = defaultdict(lambda: {'scores': [], 'paper_count': 0})
        
        for idx, row in df.iterrows():
            paper_id = str(row[id_col]) if pd.notna(row.get(id_col)) else f"paper_{idx}"
            journal = row[journal_col] if pd.notna(row.get(journal_col)) else "Unknown"
            
            # è·å–å…³é”®è¯
            keywords = []
            if keywords_col in row and pd.notna(row[keywords_col]):
                keywords = clean_keywords(row[keywords_col])
            
            if len(keywords) < 2:
                continue
            
            # ç”Ÿæˆæ‰€æœ‰å…³é”®è¯å¯¹å¹¶è®¡ç®—æ–°é¢–æ€§
            pairs = list(combinations(keywords, 2))
            novel_pairs = 0
            
            for pair in pairs:
                norm_pair = tuple(sorted(pair))
                first_year = pair_timeline.get(norm_pair)
                
                # åˆ¤æ–­æ˜¯å¦æ–°é¢–ï¼šä»æœªå‡ºç°è¿‡æˆ–å‡ºç°æ—¶é—´å¾ˆè¿‘
                if first_year is None or (current_year - first_year) <= threshold_years:
                    novel_pairs += 1
            
            # è®¡ç®—æ–°é¢–æ€§å¾—åˆ†
            if pairs:
                score = novel_pairs / len(pairs)
                paper_results.append({
                    'paper_id': paper_id,
                    'journal': journal,
                    'novelty_score': score,
                    'keyword_count': len(keywords),
                    'novel_pairs': novel_pairs,
                    'total_pairs': len(pairs)
                })
                
                # æŒ‰æœŸåˆŠèšåˆ
                journal_results[journal]['scores'].append(score)
                journal_results[journal]['paper_count'] += 1
        
        # è®¡ç®—æœŸåˆŠå¹³å‡æ–°é¢–æ€§
        journal_scores = {}
        for journal, data in journal_results.items():
            if data['scores']:
                journal_scores[journal] = {
                    'novelty_score': np.mean(data['scores']),
                    'paper_count': data['paper_count'],
                    'score_std': np.std(data['scores']) if len(data['scores']) > 1 else 0
                }
        
        log(f"æ–°é¢–æ€§è®¡ç®—å®Œæˆ: {len(paper_results)} ç¯‡è®ºæ–‡, {len(journal_scores)} ç§æœŸåˆŠ")
        return journal_scores

    def generate_outputs(self, journal_scores):
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶"""
        if not journal_scores:
            log("è­¦å‘Š: æ²¡æœ‰è®¡ç®—åˆ°æœŸåˆŠæ–°é¢–æ€§å¾—åˆ†")
            return
        
        # 1. åˆ›å»ºå¾—åˆ†åˆ—è¡¨DataFrame
        score_list = []
        for journal, scores in journal_scores.items():
            percent_score = scores.get('percent_score', 0)
            score_list.append({
                'æœŸåˆŠåç§°': journal,
                'æ–°é¢–æ€§å¾—åˆ†': scores['novelty_score'],  # ä¿ç•™åŸå§‹å°æ•°ä½æ•°
                'ç™¾åˆ†åˆ¶å¾—åˆ†': percent_score,  # ä¿ç•™åŸå§‹å°æ•°ä½æ•°
                'è®ºæ–‡æ•°é‡': scores['paper_count'],
                'å¾—åˆ†æ ‡å‡†å·®': scores.get('score_std', 0)
            })
        
        score_df = pd.DataFrame(score_list)
        score_df = score_df.sort_values('ç™¾åˆ†åˆ¶å¾—åˆ†', ascending=False).reset_index(drop=True)
        score_df.insert(0, 'æ’å', range(1, len(score_df) + 1))
        
        # ä¿å­˜CSVï¼ˆä¸æ ¼å¼åŒ–ï¼Œä¿ç•™åŸå§‹å°æ•°ä½æ•°ï¼‰
        csv_path = self.output_dir / "journal_novelty_scores.csv"
        score_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        log(f"ğŸ“„ å¾—åˆ†åˆ—è¡¨å·²ä¿å­˜: {csv_path}")
        
        # 2. ç”Ÿæˆç¾è§‚çš„ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾
        self.create_percent_chart(score_df)

    def create_percent_chart(self, data):
        """åˆ›å»ºç¾è§‚çš„ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾"""
        if len(data) == 0:
            log("æ²¡æœ‰æ•°æ®å¯ç”Ÿæˆå›¾è¡¨")
            return
        
        # è®¾ç½®å›¾è¡¨å°ºå¯¸ï¼ˆæ ¹æ®æœŸåˆŠæ•°é‡è°ƒæ•´é«˜åº¦ï¼‰
        n_journals = len(data)
        fig_height = max(6, n_journals * 0.35)
        fig, ax = plt.subplots(figsize=(14, fig_height))
        
        # ä½¿ç”¨æ¸å˜è‰²ï¼ˆæ©™è‰²ç³»ï¼Œé€‚åˆåˆ›æ–°ä¸»é¢˜ï¼‰
        colors = plt.cm.autumn(np.linspace(0.3, 0.9, n_journals))
        
        # åˆ›å»ºæ°´å¹³æŸ±çŠ¶å›¾
        y_positions = np.arange(n_journals)
        bars = ax.barh(y_positions, data['ç™¾åˆ†åˆ¶å¾—åˆ†'], 
                      color=colors, 
                      alpha=0.85,
                      height=0.7,
                      edgecolor='white',
                      linewidth=1.5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        max_percent = max(data['ç™¾åˆ†åˆ¶å¾—åˆ†'])
        for i, (bar, percent_score, journal_name, paper_count, raw_score) in enumerate(
            zip(bars, data['ç™¾åˆ†åˆ¶å¾—åˆ†'], data['æœŸåˆŠåç§°'], data['è®ºæ–‡æ•°é‡'], data['æ–°é¢–æ€§å¾—åˆ†'])):
            
            # ç™¾åˆ†åˆ¶å¾—åˆ†æ ‡ç­¾ï¼ˆå³ä¾§ï¼‰
            label_x = bar.get_width() + max_percent * 0.02
            ax.text(label_x, bar.get_y() + bar.get_height()/2,
                   f'{percent_score:.1f}åˆ†', 
                   ha='left', va='center',
                   fontsize=10, fontweight='bold',
                   color='#2c3e50',
                   bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))
            
            # åœ¨æŸ±å­å·¦ä¾§æ˜¾ç¤ºæœŸåˆŠåç§°å’Œæ’å
            short_name = journal_name[:28] + "..." if len(journal_name) > 28 else journal_name
            rank_text = f"{i+1}. {short_name}"
            ax.text(-max_percent * 0.02, bar.get_y() + bar.get_height()/2,
                   rank_text,
                   ha='right', va='center',
                   fontsize=9.5, fontweight='medium',
                   color='#34495e')
            
            # åœ¨æŸ±å­å†…éƒ¨æ˜¾ç¤ºè®ºæ–‡æ•°é‡å’ŒåŸå§‹æ–°é¢–æ€§å¾—åˆ†
            if bar.get_width() > max_percent * 0.1:
                inner_text = f"{paper_count}ç¯‡\n{raw_score:.4f}"
                ax.text(bar.get_width()/2, bar.get_y() + bar.get_height()/2,
                       inner_text,
                       ha='center', va='center',
                       fontsize=8, color='white',
                       fontweight='bold')
        
        # è®¾ç½®yè½´
        ax.set_yticks(y_positions)
        ax.set_yticklabels([])
        
        # è®¾ç½®xè½´
        ax.set_xlabel('ç™¾åˆ†åˆ¶å¾—åˆ† (åˆ†)', fontsize=11, fontweight='bold', color='#2c3e50')
        
        # åŠ¨æ€è®¾ç½®xè½´èŒƒå›´
        max_score = max(data['ç™¾åˆ†åˆ¶å¾—åˆ†'])
        ax.set_xlim([0, max_score * 1.15])
        
        # è®¾ç½®æ ‡é¢˜
        ax.set_title('æœŸåˆŠæ–°é¢–æ€§æ’å - ç™¾åˆ†åˆ¶å¾—åˆ† (å¾—åˆ† = æ–°é¢–æ€§å¾—åˆ† Ã— 100)', 
                    fontsize=14, fontweight='bold', pad=20, color='#2c3e50')
        
        # ç¾åŒ–æ ·å¼
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['left'].set_visible(False)
        ax.spines['bottom'].set_color('#95a5a6')
        
        # ç½‘æ ¼çº¿
        ax.grid(axis='x', linestyle='--', alpha=0.25, color='#bdc3c7')
        
        # åè½¬yè½´ï¼ˆä»é«˜åˆ°ä½ï¼‰
        ax.invert_yaxis()
        
        # èƒŒæ™¯è‰²
        ax.set_facecolor('#f8f9fa')
        fig.patch.set_facecolor('white')
        
        # åˆ»åº¦çº¿
        ax.tick_params(axis='x', colors='#7f8c8d')
        ax.tick_params(axis='y', length=0)
        
        # æ·»åŠ å…¬å¼è¯´æ˜
        formula_text = "å¾—åˆ†å…¬å¼: ç™¾åˆ†åˆ¶å¾—åˆ† = æ–°é¢–æ€§å¾—åˆ† Ã— 100"
        ax.text(0.5, -0.05, formula_text,
               transform=ax.transAxes,
               ha='center', va='center',
               fontsize=9, style='italic',
               color='#7f8c8d')
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        plt.subplots_adjust(bottom=0.12)
        
        # ä¿å­˜å›¾ç‰‡
        img_path = self.output_dir / "journal_novelty_percent_chart.png"
        plt.savefig(img_path, dpi=300, bbox_inches='tight', 
                    facecolor=fig.get_facecolor(), edgecolor='none')
        plt.close()
        
        log(f"ğŸ“Š ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾å·²ä¿å­˜: {img_path}")

def main():
    try:
        analyzer = NoveltyAnalyzer()
        results = analyzer.run_analysis()
        
        # æ˜¾ç¤ºç»“æœ
        if results:
            print("\n" + "="*85)
            print("æœŸåˆŠæ–°é¢–æ€§å¾—åˆ†ï¼ˆç™¾åˆ†åˆ¶å¾—åˆ† = æ–°é¢–æ€§å¾—åˆ† Ã— 600ï¼‰")
            print("="*85)
            
            # åˆ›å»ºä¸´æ—¶DataFrameç”¨äºæ˜¾ç¤º
            display_list = []
            for journal, scores in results.items():
                display_list.append({
                    'æœŸåˆŠåç§°': journal,
                    'ç™¾åˆ†åˆ¶å¾—åˆ†': scores.get('percent_score', 0),
                    'æ–°é¢–æ€§å¾—åˆ†': scores.get('novelty_score', 0),
                    'è®ºæ–‡æ•°é‡': scores.get('paper_count', 0)
                })
            
            display_df = pd.DataFrame(display_list)
            display_df = display_df.sort_values('ç™¾åˆ†åˆ¶å¾—åˆ†', ascending=False).reset_index(drop=True)
            
            print(f"{'æ’å':^4} | {'æœŸåˆŠåç§°':^40} | {'ç™¾åˆ†åˆ¶å¾—åˆ†':^12} | {'æ–°é¢–æ€§å¾—åˆ†':^12} | {'è®ºæ–‡æ•°':^8}")
            print("-"*85)
            
            for i, row in display_df.iterrows():
                journal_name = row['æœŸåˆŠåç§°']
                if len(journal_name) > 38:
                    journal_name = journal_name[:35] + "..."
                
                print(f"{i+1:^4} | {journal_name:^40} | {row['ç™¾åˆ†åˆ¶å¾—åˆ†']:^12.1f} | "
                      f"{row['æ–°é¢–æ€§å¾—åˆ†']:^12.4f} | {row['è®ºæ–‡æ•°é‡']:^8}")
            
            print("="*85)
            
    except Exception as e:
        print(f"ç¨‹åºé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()