# -*- coding: utf-8 -*-
"""
python_analysis/novelty_analyzer.py
<<<<<<< HEAD
æœŸåˆŠæ–°é¢–æ€§æŒ‡æ•°åˆ†æç³»ç»Ÿ - ä¿®æ­£ç‰ˆ
ç™¾åˆ†åˆ¶å¾—åˆ† = æ–°é¢–æ€§å¾—åˆ† Ã— 600
è¾“å‡ºï¼šæ–°é¢–æ€§å¾—åˆ†åˆ—è¡¨ + ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾
"""
import json
import pandas as pd
import numpy as np
import ast
import matplotlib.pyplot as plt
from pathlib import Path
from collections import defaultdict, Counter
from itertools import combinations

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib.font_manager as fm
try:
    font_path = "C:/Windows/Fonts/simhei.ttf"
    if Path(font_path).exists():
        fm.fontManager.addfont(font_path)
        font_name = fm.FontProperties(fname=font_path).get_name()
        plt.rcParams['font.sans-serif'] = [font_name]
    plt.rcParams['axes.unicode_minus'] = False
except:
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).resolve().parent.parent / 'config.json'
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    return config.get("novelty", {})

def log(msg):
    print(f"[novelty] {msg}")

def clean_keywords(keywords_str):
    """æ¸…æ´—å…³é”®è¯"""
    if pd.isna(keywords_str):
        return []
    
    if isinstance(keywords_str, str):
        if keywords_str.startswith('[') and keywords_str.endswith(']'):
            try:
                return ast.literal_eval(keywords_str)
            except:
                pass
        
        # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²
        separators = [';', ',', '|', 'ã€']
        for sep in separators:
            if sep in keywords_str:
                return [k.strip().lower() for k in keywords_str.split(sep) if k.strip()]
        
        return [keywords_str.strip().lower()]
    
    return []

def calculate_percent_score(novelty_score):
    """è®¡ç®—ç™¾åˆ†åˆ¶å¾—åˆ†ï¼šæ–°é¢–æ€§å¾—åˆ† Ã— 600"""
    if pd.isna(novelty_score):
        return 0.0
    percent_score = novelty_score * 600
    return percent_score  # ä¿ç•™åŸå§‹å°æ•°ä½æ•°

class NoveltyAnalyzer:
    def __init__(self, config=None):
        self.config = config or load_config()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config['output']['novelty_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir = output_dir

    def run_analysis(self):
        """è¿è¡Œæ–°é¢–æ€§åˆ†æ"""
        try:
            log("=" * 50)
            log("å¼€å§‹æœŸåˆŠæ–°é¢–æ€§åˆ†æï¼ˆUzziç»„åˆæ–¹æ³•ï¼‰")
            log("=" * 50)
            
            # åŠ è½½æ•°æ®
            project_root = Path(__file__).resolve().parent.parent
            data_config = self.config.get('data_sources', {})
            
            # åŠ è½½èƒŒæ™¯æ•°æ®ï¼ˆæ„å»ºæ—¶é—´çº¿ï¼‰
            bg_path = project_root / data_config['all_data']
            tg_path = project_root / data_config['target_data']
            
            log(f"ğŸ“‚ åŠ è½½æ•°æ®...")
            log(f"  èƒŒæ™¯æ•°æ®: {bg_path}")
            log(f"  ç›®æ ‡æ•°æ®: {tg_path}")
            
            background_df = pd.read_csv(bg_path)
            target_df = pd.read_csv(tg_path)
            
            # è·å–åˆ—å
            id_col = self.config['columns']['id']
            journal_col = self.config['columns']['journal']
            keywords_col = self.config['columns']['keywords']
            year_col = self.config['columns']['year']
            
            log(f"ä½¿ç”¨åˆ—: ID={id_col}, æœŸåˆŠ={journal_col}, å…³é”®è¯={keywords_col}, å¹´ä»½={year_col}")
            
            # é˜¶æ®µ1: ä½¿ç”¨èƒŒæ™¯æ•°æ®æ„å»ºå…³é”®è¯å¯¹æ—¶é—´çº¿
            log("\nğŸ“Š æ„å»ºå…³é”®è¯å¯¹æ—¶é—´çº¿ï¼ˆèƒŒæ™¯æ•°æ®ï¼‰...")
            bg_pair_timeline = self._build_pair_timeline(background_df, id_col, keywords_col, year_col)
            
            # é˜¶æ®µ2: è®¡ç®—ç›®æ ‡æ•°æ®çš„æ–°é¢–æ€§
            log("ğŸ¯ è®¡ç®—ç›®æ ‡æœŸåˆŠæ–°é¢–æ€§...")
            journal_scores = self._calculate_target_novelty(target_df, bg_pair_timeline, 
                                                          id_col, journal_col, keywords_col, year_col)
            
            # é˜¶æ®µ3: è®¡ç®—ç™¾åˆ†åˆ¶å¾—åˆ†
            log("ğŸ“ˆ è®¡ç®—ç™¾åˆ†åˆ¶å¾—åˆ†...")
            for journal in journal_scores:
                if 'novelty_score' in journal_scores[journal]:
                    raw_score = journal_scores[journal]['novelty_score']
                    journal_scores[journal]['percent_score'] = calculate_percent_score(raw_score)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
            self.generate_outputs(journal_scores)
            
            log("\nâœ… åˆ†æå®Œæˆï¼")
            log(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
            log(f"  1. journal_novelty_scores.csv - æœŸåˆŠæ–°é¢–æ€§å¾—åˆ†åˆ—è¡¨")
            log(f"  2. journal_novelty_percent_chart.png - ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾")
            
            return journal_scores
            
        except Exception as e:
            log(f"é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    def _build_pair_timeline(self, df, id_col, keywords_col, year_col):
        """æ„å»ºå…³é”®è¯å¯¹é¦–æ¬¡å‡ºç°æ—¶é—´çº¿"""
        pair_year = defaultdict(lambda: float('inf'))
        
        for idx, row in df.iterrows():
            paper_id = str(row[id_col]) if pd.notna(row.get(id_col)) else f"paper_{idx}"
            
            # è·å–å…³é”®è¯
            keywords = []
            if keywords_col in row and pd.notna(row[keywords_col]):
                keywords = clean_keywords(row[keywords_col])
            
            if len(keywords) < 2:
                continue
            
            # è·å–å¹´ä»½
            if year_col in row and pd.notna(row[year_col]):
                try:
                    year = int(float(row[year_col]))
                except:
                    continue
            else:
                continue
            
            # ç”Ÿæˆæ‰€æœ‰å…³é”®è¯å¯¹
            for pair in combinations(keywords, 2):
                norm_pair = tuple(sorted(pair))
                if year < pair_year[norm_pair]:
                    pair_year[norm_pair] = year
        
        log(f"æ—¶é—´çº¿æ„å»ºå®Œæˆ: {len(pair_year)} ä¸ªå…³é”®è¯å¯¹")
        return dict(pair_year)

    def _calculate_target_novelty(self, df, pair_timeline, id_col, journal_col, keywords_col, year_col):
        """è®¡ç®—ç›®æ ‡æ•°æ®çš„æ–°é¢–æ€§"""
        # ä»é…ç½®è·å–å‚æ•°
        threshold_years = self.config.get('parameters', {}).get('novel_threshold_years', 1)
        
        # ç¡®å®šå½“å‰å¹´ä»½ï¼ˆä½¿ç”¨ç›®æ ‡æ•°æ®çš„æœ€æ–°å¹´ä»½ï¼‰
        current_year = df[year_col].max() if year_col in df.columns else pd.Timestamp.now().year
        if pd.isna(current_year):
            current_year = pd.Timestamp.now().year
        else:
            current_year = int(current_year)
        
        log(f"æ–°é¢–æ€§å‚æ•°: é˜ˆå€¼={threshold_years}å¹´, åŸºå‡†å¹´ä»½={current_year}")
        
        # æŒ‰è®ºæ–‡è®¡ç®—æ–°é¢–æ€§
        paper_results = []
        journal_results = defaultdict(lambda: {'scores': [], 'paper_count': 0})
        
        for idx, row in df.iterrows():
            paper_id = str(row[id_col]) if pd.notna(row.get(id_col)) else f"paper_{idx}"
            journal = row[journal_col] if pd.notna(row.get(journal_col)) else "Unknown"
            
            # è·å–å…³é”®è¯
            keywords = []
            if keywords_col in row and pd.notna(row[keywords_col]):
                keywords = clean_keywords(row[keywords_col])
            
            if len(keywords) < 2:
                continue
            
            # ç”Ÿæˆæ‰€æœ‰å…³é”®è¯å¯¹å¹¶è®¡ç®—æ–°é¢–æ€§
            pairs = list(combinations(keywords, 2))
            novel_pairs = 0
            
            for pair in pairs:
                norm_pair = tuple(sorted(pair))
                first_year = pair_timeline.get(norm_pair)
                
                # åˆ¤æ–­æ˜¯å¦æ–°é¢–ï¼šä»æœªå‡ºç°è¿‡æˆ–å‡ºç°æ—¶é—´å¾ˆè¿‘
                if first_year is None or (current_year - first_year) <= threshold_years:
                    novel_pairs += 1
            
            # è®¡ç®—æ–°é¢–æ€§å¾—åˆ†
            if pairs:
                score = novel_pairs / len(pairs)
                paper_results.append({
                    'paper_id': paper_id,
                    'journal': journal,
                    'novelty_score': score,
                    'keyword_count': len(keywords),
                    'novel_pairs': novel_pairs,
                    'total_pairs': len(pairs)
                })
                
                # æŒ‰æœŸåˆŠèšåˆ
                journal_results[journal]['scores'].append(score)
                journal_results[journal]['paper_count'] += 1
        
        # è®¡ç®—æœŸåˆŠå¹³å‡æ–°é¢–æ€§
        journal_scores = {}
        for journal, data in journal_results.items():
            if data['scores']:
                journal_scores[journal] = {
                    'novelty_score': np.mean(data['scores']),
                    'paper_count': data['paper_count'],
                    'score_std': np.std(data['scores']) if len(data['scores']) > 1 else 0
                }
        
        log(f"æ–°é¢–æ€§è®¡ç®—å®Œæˆ: {len(paper_results)} ç¯‡è®ºæ–‡, {len(journal_scores)} ç§æœŸåˆŠ")
        return journal_scores

    def generate_outputs(self, journal_scores):
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶"""
        if not journal_scores:
            log("è­¦å‘Š: æ²¡æœ‰è®¡ç®—åˆ°æœŸåˆŠæ–°é¢–æ€§å¾—åˆ†")
            return
        
        # 1. åˆ›å»ºå¾—åˆ†åˆ—è¡¨DataFrame
        score_list = []
        for journal, scores in journal_scores.items():
            percent_score = scores.get('percent_score', 0)
            score_list.append({
                'æœŸåˆŠåç§°': journal,
                'æ–°é¢–æ€§å¾—åˆ†': scores['novelty_score'],  # ä¿ç•™åŸå§‹å°æ•°ä½æ•°
                'ç™¾åˆ†åˆ¶å¾—åˆ†': percent_score,  # ä¿ç•™åŸå§‹å°æ•°ä½æ•°
                'è®ºæ–‡æ•°é‡': scores['paper_count'],
                'å¾—åˆ†æ ‡å‡†å·®': scores.get('score_std', 0)
            })
        
        score_df = pd.DataFrame(score_list)
        score_df = score_df.sort_values('ç™¾åˆ†åˆ¶å¾—åˆ†', ascending=False).reset_index(drop=True)
        score_df.insert(0, 'æ’å', range(1, len(score_df) + 1))
        
        # ä¿å­˜CSVï¼ˆä¸æ ¼å¼åŒ–ï¼Œä¿ç•™åŸå§‹å°æ•°ä½æ•°ï¼‰
        csv_path = self.output_dir / "journal_novelty_scores.csv"
        score_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        log(f"ğŸ“„ å¾—åˆ†åˆ—è¡¨å·²ä¿å­˜: {csv_path}")
        
        # 2. ç”Ÿæˆç¾è§‚çš„ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾
        self.create_percent_chart(score_df)

    def create_percent_chart(self, data):
        """åˆ›å»ºç¾è§‚çš„ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾"""
        if len(data) == 0:
            log("æ²¡æœ‰æ•°æ®å¯ç”Ÿæˆå›¾è¡¨")
            return
        
        # è®¾ç½®å›¾è¡¨å°ºå¯¸ï¼ˆæ ¹æ®æœŸåˆŠæ•°é‡è°ƒæ•´é«˜åº¦ï¼‰
        n_journals = len(data)
        fig_height = max(6, n_journals * 0.35)
        fig, ax = plt.subplots(figsize=(14, fig_height))
        
        # ä½¿ç”¨æ¸å˜è‰²ï¼ˆæ©™è‰²ç³»ï¼Œé€‚åˆåˆ›æ–°ä¸»é¢˜ï¼‰
        colors = plt.cm.autumn(np.linspace(0.3, 0.9, n_journals))
        
        # åˆ›å»ºæ°´å¹³æŸ±çŠ¶å›¾
        y_positions = np.arange(n_journals)
        bars = ax.barh(y_positions, data['ç™¾åˆ†åˆ¶å¾—åˆ†'], 
                      color=colors, 
                      alpha=0.85,
                      height=0.7,
                      edgecolor='white',
                      linewidth=1.5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        max_percent = max(data['ç™¾åˆ†åˆ¶å¾—åˆ†'])
        for i, (bar, percent_score, journal_name, paper_count, raw_score) in enumerate(
            zip(bars, data['ç™¾åˆ†åˆ¶å¾—åˆ†'], data['æœŸåˆŠåç§°'], data['è®ºæ–‡æ•°é‡'], data['æ–°é¢–æ€§å¾—åˆ†'])):
            
            # ç™¾åˆ†åˆ¶å¾—åˆ†æ ‡ç­¾ï¼ˆå³ä¾§ï¼‰
            label_x = bar.get_width() + max_percent * 0.02
            ax.text(label_x, bar.get_y() + bar.get_height()/2,
                   f'{percent_score:.1f}åˆ†', 
                   ha='left', va='center',
                   fontsize=10, fontweight='bold',
                   color='#2c3e50',
                   bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))
            
            # åœ¨æŸ±å­å·¦ä¾§æ˜¾ç¤ºæœŸåˆŠåç§°å’Œæ’å
            short_name = journal_name[:28] + "..." if len(journal_name) > 28 else journal_name
            rank_text = f"{i+1}. {short_name}"
            ax.text(-max_percent * 0.02, bar.get_y() + bar.get_height()/2,
                   rank_text,
                   ha='right', va='center',
                   fontsize=9.5, fontweight='medium',
                   color='#34495e')
            
            # åœ¨æŸ±å­å†…éƒ¨æ˜¾ç¤ºè®ºæ–‡æ•°é‡å’ŒåŸå§‹æ–°é¢–æ€§å¾—åˆ†
            if bar.get_width() > max_percent * 0.1:
                inner_text = f"{paper_count}ç¯‡\n{raw_score:.4f}"
                ax.text(bar.get_width()/2, bar.get_y() + bar.get_height()/2,
                       inner_text,
                       ha='center', va='center',
                       fontsize=8, color='white',
                       fontweight='bold')
        
        # è®¾ç½®yè½´
        ax.set_yticks(y_positions)
        ax.set_yticklabels([])
        
        # è®¾ç½®xè½´
        ax.set_xlabel('ç™¾åˆ†åˆ¶å¾—åˆ† (åˆ†)', fontsize=11, fontweight='bold', color='#2c3e50')
        
        # åŠ¨æ€è®¾ç½®xè½´èŒƒå›´
        max_score = max(data['ç™¾åˆ†åˆ¶å¾—åˆ†'])
        ax.set_xlim([0, max_score * 1.15])
        
        # è®¾ç½®æ ‡é¢˜
        ax.set_title('æœŸåˆŠæ–°é¢–æ€§æ’å - ç™¾åˆ†åˆ¶å¾—åˆ† (å¾—åˆ† = æ–°é¢–æ€§å¾—åˆ† Ã— 100)', 
                    fontsize=14, fontweight='bold', pad=20, color='#2c3e50')
        
        # ç¾åŒ–æ ·å¼
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['left'].set_visible(False)
        ax.spines['bottom'].set_color('#95a5a6')
        
        # ç½‘æ ¼çº¿
        ax.grid(axis='x', linestyle='--', alpha=0.25, color='#bdc3c7')
        
        # åè½¬yè½´ï¼ˆä»é«˜åˆ°ä½ï¼‰
        ax.invert_yaxis()
        
        # èƒŒæ™¯è‰²
        ax.set_facecolor('#f8f9fa')
        fig.patch.set_facecolor('white')
        
        # åˆ»åº¦çº¿
        ax.tick_params(axis='x', colors='#7f8c8d')
        ax.tick_params(axis='y', length=0)
        
        # æ·»åŠ å…¬å¼è¯´æ˜
        formula_text = "å¾—åˆ†å…¬å¼: ç™¾åˆ†åˆ¶å¾—åˆ† = æ–°é¢–æ€§å¾—åˆ† Ã— 100"
        ax.text(0.5, -0.05, formula_text,
               transform=ax.transAxes,
               ha='center', va='center',
               fontsize=9, style='italic',
               color='#7f8c8d')
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        plt.subplots_adjust(bottom=0.12)
        
        # ä¿å­˜å›¾ç‰‡
        img_path = self.output_dir / "journal_novelty_percent_chart.png"
        plt.savefig(img_path, dpi=300, bbox_inches='tight', 
                    facecolor=fig.get_facecolor(), edgecolor='none')
        plt.close()
        
        log(f"ğŸ“Š ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾å·²ä¿å­˜: {img_path}")

def main():
    try:
        analyzer = NoveltyAnalyzer()
        results = analyzer.run_analysis()
        
        # æ˜¾ç¤ºç»“æœ
        if results:
            print("\n" + "="*85)
            print("æœŸåˆŠæ–°é¢–æ€§å¾—åˆ†ï¼ˆç™¾åˆ†åˆ¶å¾—åˆ† = æ–°é¢–æ€§å¾—åˆ† Ã— 600ï¼‰")
            print("="*85)
            
            # åˆ›å»ºä¸´æ—¶DataFrameç”¨äºæ˜¾ç¤º
            display_list = []
            for journal, scores in results.items():
                display_list.append({
                    'æœŸåˆŠåç§°': journal,
                    'ç™¾åˆ†åˆ¶å¾—åˆ†': scores.get('percent_score', 0),
                    'æ–°é¢–æ€§å¾—åˆ†': scores.get('novelty_score', 0),
                    'è®ºæ–‡æ•°é‡': scores.get('paper_count', 0)
                })
            
            display_df = pd.DataFrame(display_list)
            display_df = display_df.sort_values('ç™¾åˆ†åˆ¶å¾—åˆ†', ascending=False).reset_index(drop=True)
            
            print(f"{'æ’å':^4} | {'æœŸåˆŠåç§°':^40} | {'ç™¾åˆ†åˆ¶å¾—åˆ†':^12} | {'æ–°é¢–æ€§å¾—åˆ†':^12} | {'è®ºæ–‡æ•°':^8}")
            print("-"*85)
            
            for i, row in display_df.iterrows():
                journal_name = row['æœŸåˆŠåç§°']
                if len(journal_name) > 38:
                    journal_name = journal_name[:35] + "..."
                
                print(f"{i+1:^4} | {journal_name:^40} | {row['ç™¾åˆ†åˆ¶å¾—åˆ†']:^12.1f} | "
                      f"{row['æ–°é¢–æ€§å¾—åˆ†']:^12.4f} | {row['è®ºæ–‡æ•°é‡']:^8}")
            
            print("="*85)
            
    except Exception as e:
        print(f"ç¨‹åºé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
=======
æœŸåˆŠæ–°é¢–æ€§æŒ‡æ•°åˆ†æç³»ç»Ÿï¼ˆåŸºäº Uzzi et al., Science 2013 çš„ç»„åˆæ–°é¢–æ€§æ–¹æ³•ï¼‰
"""

# === åŸºç¡€åº“ ===
import os
import json
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
from itertools import combinations
import ast

# === å¯è§†åŒ–æ”¯æŒ ===
import matplotlib.pyplot as plt
import seaborn as sns

# === è®¾ç½® ===
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
pd.options.mode.chained_assignment = None


class NoveltyAnalyzer:
    """
    ç»„åˆæ–°é¢–æ€§åˆ†æå™¨ï¼ˆCombination Noveltyï¼‰
    åŸºäºå…³é”®è¯å…±ç°æ¨¡å¼è¯†åˆ«â€œå‰æ‰€æœªæœ‰â€çš„çŸ¥è¯†ç»„åˆ
    """

    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
        self.paper_keywords = {}           # DOI -> [keywords]
        self.keyword_pairs_first_seen = {} # (k1,k2) -> first_year
        self.results = {}

    def analyze(self) -> dict:
        """
        æ‰§è¡Œç»„åˆæ–°é¢–æ€§åˆ†æ
        
        æµç¨‹ï¼š
        1. æå–æ¯ç¯‡è®ºæ–‡çš„å…³é”®è¯
        2. æ„å»ºæ‰€æœ‰å…³é”®è¯å¯¹çš„å†å²é¦–æ¬¡å‡ºç°å¹´ä»½
        3. è®¡ç®—æ¯ç¯‡è®ºæ–‡ä¸­â€œé¦–æ¬¡å‡ºç°â€çš„æ–°ç»„åˆæ¯”ä¾‹
        4. æŒ‰æœŸåˆŠèšåˆä¸ºå¹³å‡æ–°é¢–æ€§å¾—åˆ†
        
        Returns:
            {æœŸåˆŠå: æ–°é¢–æ€§å¾—åˆ†}
        """
        print("ğŸ” å¼€å§‹ç»„åˆæ–°é¢–æ€§åˆ†æï¼ˆUzzi æ–¹æ³•ï¼‰...")

        # Step 1: æå–å…³é”®è¯
        self._extract_paper_keywords()

        # Step 2: æ„å»ºå…¨å±€å…³é”®è¯å¯¹æ—¶é—´çº¿
        self._build_keyword_pair_timeline()

        # Step 3: è®¡ç®—æ¯ç¯‡è®ºæ–‡çš„æ–°é¢–æ€§å¾—åˆ†
        paper_novelty_df = self._calculate_paper_combination_novelty()

        # Step 4: æŒ‰æœŸåˆŠè®¡ç®—å¹³å‡å¾—åˆ†
        journal_novelty = self._aggregate_to_journal(paper_novelty_df)

        self.results = journal_novelty
        return journal_novelty

    def _extract_paper_keywords(self):
        """æå–å¹¶æ¸…æ´—å…³é”®è¯"""
        print("ğŸ“ æå–è®ºæ–‡å…³é”®è¯...")
        stop_words = {
            'review', 'studies', 'study', 'analysis', 'method', 'methods',
            'approach', 'approaches', 'framework', 'model', 'system',
            'based', 'using', 'via', 'case study', 'research', 'development'
        }

        for _, row in self.df.iterrows():
            paper_id = row['DOI'] if pd.notna(row['DOI']) else f"paper_{_}"
            keywords = []

            if pd.notna(row.get('Keywords')):
                word_content = str(row['Keywords']).strip()
                try:
                    if word_content.startswith('[') and word_content.endswith(']'):
                        raw_list = ast.literal_eval(word_content)
                        keywords = [str(kw).strip().lower() for kw in raw_list if str(kw).strip()]
                    else:
                        keywords = [kw.strip().lower() for kw in word_content.replace(';', ',').split(',') if kw.strip()]
                except (ValueError, SyntaxError):
                    keywords = [word_content.lower()] if word_content else []

            # æ¸…æ´—ï¼šå»åœç”¨è¯ã€å»ç©ºæ ¼ã€æ ‡å‡†åŒ–
            keywords = [kw for kw in keywords if kw not in stop_words and len(kw) > 1]
            self.paper_keywords[paper_id] = sorted(set(keywords))  # æ’åºä¾¿äº pair ä¸€è‡´

        total_papers = len(self.paper_keywords)
        papers_with_kw = sum(1 for kw in self.paper_keywords.values() if kw)
        total_keywords = sum(len(kw) for kw in self.paper_keywords.values())
        unique_keywords = len(set(kw for kws in self.paper_keywords.values() for kw in kws))

        print(f"âœ… å…³é”®è¯æå–å®Œæˆ: {papers_with_kw}/{total_papers} ç¯‡æœ‰å…³é”®è¯")
        print(f"   å…± {unique_keywords} ä¸ªç‹¬ç‰¹å…³é”®è¯, æ€»å…±å‡ºç° {total_keywords} æ¬¡")

    def _build_keyword_pair_timeline(self):
        """æ„å»ºæ‰€æœ‰å…³é”®è¯å¯¹çš„é¦–æ¬¡å‡ºç°å¹´ä»½"""
        print("ğŸ“… æ„å»ºå…³é”®è¯ç»„åˆæ—¶é—´çº¿...")
        pair_first_year = defaultdict(lambda: float('inf'))

        missing_year_count = 0
        for paper_id, keywords in self.paper_keywords.items():
            if len(keywords) < 2:
                continue

            year = self._get_paper_year(paper_id)
            if not year:
                missing_year_count += 1
                continue

            # ç”Ÿæˆæ‰€æœ‰æ— åºä¸¤ä¸¤ç»„åˆ
            for pair in combinations(keywords, 2):
                # è§„èŒƒåŒ–é¡ºåºï¼šå­—å…¸åºï¼Œç¡®ä¿ ('a','b') == ('b','a')
                norm_pair = tuple(sorted(pair))
                if year < pair_first_year[norm_pair]:
                    pair_first_year[norm_pair] = year

        self.keyword_pairs_first_seen = dict(pair_first_year)
        print(f"âœ… æ„å»ºå®Œæˆ: å…± {len(pair_first_year)} ä¸ªå…³é”®è¯å¯¹ç»„åˆ")
        if missing_year_count:
            print(f"âš ï¸  {missing_year_count} ç¯‡è®ºæ–‡ç¼ºå°‘å¹´ä»½ä¿¡æ¯è¢«è·³è¿‡")

    def _get_paper_year(self, paper_id: str) -> int:
        """è·å–è®ºæ–‡å‡ºç‰ˆå¹´ä»½"""
        try:
            if 'DOI' not in self.df.columns:
                return None
            matched = self.df[self.df['DOI'] == paper_id]
            if len(matched) == 0:
                return None
            year_val = matched.iloc[0]['Publication Year']
            return int(year_val) if pd.notna(year_val) else None
        except Exception:
            return None

    def _calculate_paper_combination_novelty(self) -> pd.DataFrame:
        """è®¡ç®—æ¯ç¯‡è®ºæ–‡çš„ç»„åˆæ–°é¢–æ€§å¾—åˆ†"""
        print("ğŸ¯ è®¡ç®—è®ºæ–‡ç»„åˆæ–°é¢–æ€§å¾—åˆ†...")
        results = []

        current_year = self.df['Publication Year'].max() if 'Publication Year' in self.df.columns else 2024

        for paper_id, keywords in self.paper_keywords.items():
            if len(keywords) < 2:
                results.append({'DOI': paper_id, 'novelty_score': np.nan})
                continue

            pairs = list(combinations(keywords, 2))
            novel_pairs = 0
            total_pairs = len(pairs)

            for pair in pairs:
                norm_pair = tuple(sorted(pair))
                first_year = self.keyword_pairs_first_seen.get(norm_pair, None)

                if first_year is None:
                    # å®Œå…¨æœªè§çš„ç»„åˆ â†’ ææ–°é¢–
                    novel_pairs += 1
                elif abs(first_year - current_year) <= 1:  # å½“å‰æˆ–å‰ä¸€å¹´æ‰é¦–æ¬¡å‡ºç°
                    novel_pairs += 1

            # æ–°é¢–æ€§å¾—åˆ† = æ–°ç»„åˆå æ¯”
            novelty_score = novel_pairs / total_pairs if total_pairs > 0 else np.nan
            results.append({'DOI': paper_id, 'novelty_score': novelty_score})

        df_out = pd.DataFrame(results)
        valid_count = df_out['novelty_score'].notna().sum()
        print(f"âœ… å®Œæˆ: {valid_count}/{len(df_out)} ç¯‡è®ºæ–‡è·å¾—æœ‰æ•ˆå¾—åˆ†")
        return df_out

    def _aggregate_to_journal(self, paper_novelty_df: pd.DataFrame) -> dict:
        """æŒ‰æœŸåˆŠèšåˆå¹³å‡æ–°é¢–æ€§å¾—åˆ†"""
        print("ğŸ“š æŒ‰æœŸåˆŠèšåˆç»“æœ...")

        # åˆå¹¶æœŸåˆŠä¿¡æ¯
        journal_map = self.df.drop_duplicates('DOI')[['DOI', 'Source Title']].set_index('DOI')['Source Title'].to_dict()
        paper_novelty_df['Source Title'] = paper_novelty_df['DOI'].map(journal_map)

        # è¿‡æ»¤æ— æ•ˆå€¼
        valid_df = paper_novelty_df.dropna(subset=['novelty_score', 'Source Title'])

        # è®¡ç®—æ¯ç§æœŸåˆŠçš„å¹³å‡æ–°é¢–æ€§
        journal_scores = valid_df.groupby('Source Title')['novelty_score'].mean().round(6).to_dict()

        print(f"âœ… èšåˆå®Œæˆ: å…± {len(journal_scores)} ç§æœŸåˆŠ")
        return journal_scores

    def get_detailed_results(self) -> dict:
        """è¿”å›è¯¦ç»†ä¸­é—´ç»“æœï¼ˆç”¨äºè°ƒè¯•æˆ–æ‰©å±•ï¼‰"""
        return {
            'journal_novelty': self.results,
            'paper_novelty': self._calculate_paper_combination_novelty(),
            'keyword_pairs_first_seen': self.keyword_pairs_first_seen,
            'paper_keywords': self.paper_keywords
        }


def run_novelty_analysis(
    background_data_path: str = '../data/raw/data_with_citing.csv',
    target_data_path: str = '../data/raw/top10_journals_data.csv',
    output_dir: str = '../outputs/novelty'
) -> dict:
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼šä½¿ç”¨å…¨é‡æ•°æ®æ„å»ºèƒŒæ™¯çŸ¥è¯†åº“ï¼Œè¯„ä¼° Top10 æœŸåˆŠçš„æ–°é¢–æ€§
    """
    # ========== 1. æ¨å¯¼é¡¹ç›®æ ¹ç›®å½• ==========
    try:
        current_dir = os.path.dirname(__file__)
    except NameError:
        current_dir = os.getcwd()
    project_root = os.path.abspath(os.path.join(current_dir, '..'))

    bg_path = os.path.join(project_root, background_data_path.lstrip('./'))
    tg_path = os.path.join(project_root, target_data_path.lstrip('./'))
    out_dir = os.path.join(project_root, output_dir.lstrip('./'))

    os.makedirs(out_dir, exist_ok=True)

    print(f"ğŸ“ åŠ è½½èƒŒæ™¯æ•°æ®ï¼ˆå…¨é‡ï¼‰: {bg_path}")
    df_background = pd.read_csv(bg_path)

    print(f"ğŸ“ åŠ è½½ç›®æ ‡æ•°æ®ï¼ˆTop10ï¼‰: {tg_path}")
    df_target = pd.read_csv(tg_path)

    # ========== 2. ä½¿ç”¨å…¨é‡æ•°æ®æ„å»ºç»„åˆæ—¶é—´çº¿ ==========
    print("\nğŸ”„ æ­£åœ¨ä½¿ç”¨å…¨é‡æ•°æ®æ„å»ºå…³é”®è¯ç»„åˆæ—¶é—´çº¿...")
    analyzer_bg = NoveltyAnalyzer(df_background)
    
    # æˆ‘ä»¬åªéœ€è¦å®ƒçš„ _extract å’Œ _build åŠŸèƒ½
    analyzer_bg._extract_paper_keywords()
    analyzer_bg._build_keyword_pair_timeline()  # â† å…³é”®ï¼šå…¨å±€ç»„åˆæ•°æ®åº“

    print(f"âœ… å…¨å±€æ—¶é—´çº¿æ„å»ºå®Œæˆ | å…± {len(analyzer_bg.keyword_pairs_first_seen)} ä¸ªå…³é”®è¯å¯¹")

    # ========== 3. åœ¨ Top10 æ•°æ®ä¸Šè®¡ç®—æ–°é¢–æ€§ï¼ˆä½¿ç”¨å…¨å±€æ—¶é—´çº¿ï¼‰==========
    print("\nğŸ“Š å¼€å§‹è®¡ç®— Top10 æœŸåˆŠçš„ç»„åˆæ–°é¢–æ€§...")
    analyzer_target = NoveltyAnalyzer(df_target)
    analyzer_target._extract_paper_keywords()

    # æ³¨å…¥å…¨å±€ç»„åˆæ—¶é—´çº¿ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼ï¼‰
    analyzer_target.keyword_pairs_first_seen = analyzer_bg.keyword_pairs_first_seen

    # æ­£å¸¸è®¡ç®—å¾—åˆ†ï¼ˆç°åœ¨æ˜¯åŸºäºå…¨å±€èƒŒæ™¯ï¼‰
    paper_novelty_df = analyzer_target._calculate_paper_combination_novelty()
    journal_novelty = analyzer_target._aggregate_to_journal(paper_novelty_df)

    analyzer_target.results = journal_novelty

    # ========== 4. è¾“å‡ºç»“æœ ==========
    result_json = os.path.join(out_dir, 'journal_novelty_scores.json')
    with open(result_json, 'w', encoding='utf-8') as f:
        json.dump(journal_novelty, f, ensure_ascii=False, indent=2)
    print(f"âœ… å·²ä¿å­˜ JSON: {result_json}")

    result_df = pd.DataFrame(list(journal_novelty.items()), 
                           columns=['Source Title', 'novelty_score'])
    result_df = result_df.sort_values('novelty_score', ascending=False).reset_index(drop=True)

    result_csv = os.path.join(out_dir, 'journal_novelty_ranking.csv')
    result_df.to_csv(result_csv, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²ä¿å­˜ CSV: {result_csv}")

    txt_path = os.path.join(out_dir, 'top_journals_by_novelty.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        for journal in result_df['Source Title']:
            f.write(f"{journal}\n")
    print(f"âœ… å·²ä¿å­˜ TXT: {txt_path}")

    # ========== 5. å¯è§†åŒ– ==========
    try:
        top10 = result_df.head(10)
        plt.figure(figsize=(12, 8))
        bars = plt.barh(top10['Source Title'], top10['novelty_score'], color='steelblue', alpha=0.8)

        for bar, val in zip(bars, top10['novelty_score']):
            plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
                     f'{val:.4f}', ha='left', va='center', fontsize=10,
                     bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))

        plt.xlabel('ç»„åˆæ–°é¢–æ€§å¾—åˆ†ï¼ˆåŸºäºå…¨é‡èƒŒæ™¯ï¼‰')
        plt.title('æœŸåˆŠç»„åˆæ–°é¢–æ€§æ’åï¼ˆå‰10ï¼‰\n(Uzzi et al., Science 2013) - å…¨å±€çŸ¥è¯†åŸºçº¿')
        plt.gca().invert_yaxis()
        plt.tight_layout()

        img_path = os.path.join(out_dir, 'novelty_ranking.png')
        plt.savefig(img_path, dpi=150, bbox_inches='tight')
        plt.show()
        print(f"âœ… å·²ä¿å­˜å›¾è¡¨: {img_path}")
    except Exception as e:
        print(f"âš ï¸ å›¾è¡¨ä¿å­˜å¤±è´¥: {e}")

    return {
        'analyzer': analyzer_target,
        'results': journal_novelty,
        'ranking': result_df,
        'global_analyzer': analyzer_bg
    }

# ========================
# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œåˆ™æ‰§è¡Œä¸»æµç¨‹
# ========================
if __name__ == "__main__":
    run_novelty_analysis()
>>>>>>> 325a45d1acde271d3e82f31155b95d174bbec114
