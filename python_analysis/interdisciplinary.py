# -*- coding: utf-8 -*-
"""
python_analysis/interdisciplinary.py
è·¨å­¦ç§‘æ€§(TD)è®¡ç®—æ¨¡å— - ç²¾ç®€ç‰ˆ
åªè¾“å‡ºï¼šä¸¤ä¸ªæŸ±çŠ¶å›¾ + ç™¾åˆ†åˆ¶å¾—åˆ†åˆ—è¡¨
"""
import json
import pandas as pd
import numpy as np
import ast
import matplotlib.pyplot as plt
from pathlib import Path
from collections import Counter

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent.parent / 'config.json'
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    return config.get("interdisciplinary", {})

def log(msg):
    print(f"[interdisciplinary] {msg}")

class InterdisciplinaryAnalyzer:
    def __init__(self, config=None):
        self.config = config or load_config()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config['output']['interdisciplinary_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir = output_dir

    def parse_categories(self, category_str):
        """è§£æåˆ†ç±»å­—ç¬¦ä¸²"""
        if pd.isna(category_str):
            return []
        
        try:
            if isinstance(category_str, str):
                cleaned = category_str.strip()
                if cleaned.startswith('[') and cleaned.endswith(']'):
                    return ast.literal_eval(cleaned)
                else:
                    separators = [';', '|', '/']
                    for sep in separators:
                        if sep in cleaned:
                            return [cat.strip() for cat in cleaned.split(sep) if cat.strip()]
                    return [cleaned]
        except:
            pass
        
        return []

    def calculate_similarity_matrix(self, papers_data):
        """è®¡ç®—å­¦ç§‘åˆ†ç±»é—´çš„ç›¸ä¼¼æ€§çŸ©é˜µ"""
        log("è®¡ç®—å­¦ç§‘åˆ†ç±»ç›¸ä¼¼æ€§çŸ©é˜µ...")
        
        all_categories = set()
        for categories in papers_data.values():
            all_categories.update(categories)
        
        all_categories = sorted(list(all_categories))
        
        n = len(all_categories)
        co_occurrence = np.zeros((n, n))
        cat_to_idx = {cat: i for i, cat in enumerate(all_categories)}
        
        for categories in papers_data.values():
            if len(categories) < 2:
                continue
            
            for i in range(len(categories)):
                idx_i = cat_to_idx[categories[i]]
                co_occurrence[idx_i, idx_i] += 1
                
                for j in range(i+1, len(categories)):
                    idx_j = cat_to_idx[categories[j]]
                    co_occurrence[idx_i, idx_j] += 1
                    co_occurrence[idx_j, idx_i] += 1
        
        similarity = np.zeros((n, n))
        for i in range(n):
            for j in range(n):
                if i == j:
                    similarity[i, j] = 1.0
                else:
                    numerator = np.dot(co_occurrence[i, :], co_occurrence[j, :])
                    denom_i = np.sqrt(np.sum(co_occurrence[i, :] ** 2))
                    denom_j = np.sqrt(np.sum(co_occurrence[j, :] ** 2))
                    
                    if denom_i > 0 and denom_j > 0:
                        similarity[i, j] = numerator / (denom_i * denom_j)
                    else:
                        similarity[i, j] = 0.0
        
        self.all_categories = all_categories
        self.cat_to_idx = cat_to_idx
        self.similarity_matrix = similarity
        
        return similarity

    def calculate_rao_stirling_diversity(self, categories):
        """è®¡ç®—Rao-Stirlingå¤šæ ·æ€§æŒ‡æ•°"""
        if not categories or len(set(categories)) <= 1:
            return 0.0
        
        total = len(categories)
        category_probs = Counter(categories)
        
        prob_vector = np.zeros(len(self.all_categories))
        for cat, count in category_probs.items():
            if cat in self.cat_to_idx:
                prob_vector[self.cat_to_idx[cat]] = count / total
        
        diversity = 0.0
        n = len(self.all_categories)
        
        for i in range(n):
            for j in range(n):
                similarity = self.similarity_matrix[i, j]
                diversity += (1 - similarity) * prob_vector[i] * prob_vector[j]
        
        return diversity

    def calculate_td_index(self, categories):
        """è®¡ç®—TDæŒ‡æ•°"""
        diversity = self.calculate_rao_stirling_diversity(categories)
        
        if diversity > 0:
            return 1.0 / diversity
        else:
            return 1.0

    def normalize_to_percent(self, scores):
        """å½’ä¸€åŒ–åˆ°ç™¾åˆ†åˆ¶ (0-100)"""
        if isinstance(scores, (list, np.ndarray)):
            return [round(score * 5, 1) for score in scores]
        else:
            return round(scores * 5, 1)

    def run_analysis(self, background_df=None, target_df=None):
        """è¿è¡Œè·¨å­¦ç§‘æ€§åˆ†æ"""
        try:
            log("=" * 50)
            log("å¼€å§‹è·¨å­¦ç§‘æ€§åˆ†æ")
            log("=" * 50)
            
            # åŠ è½½æ•°æ®
            if background_df is None or target_df is None:
                project_root = Path(__file__).parent.parent
                all_path = project_root / self.config['data_sources']['all_data']
                target_path = project_root / self.config['data_sources']['target_data']
                
                log(f"åŠ è½½å…¨é‡æ•°æ®: {all_path}")
                log(f"åŠ è½½ç›®æ ‡æ•°æ®: {target_path}")
                
                background_df = pd.read_csv(all_path)
                target_df = pd.read_csv(target_path)
            
            # è·å–åˆ—å
            id_col = self.config['columns']['id']
            journal_col = self.config['columns']['journal']
            category_col = self.config['columns']['category']
            refs_col = self.config['columns']['refs']
            
            # æ£€æŸ¥å¿…è¦åˆ—
            required_cols = [id_col, journal_col, category_col, refs_col]
            missing_cols = [col for col in required_cols if col not in background_df.columns]
            if missing_cols:
                raise ValueError(f"èƒŒæ™¯æ•°æ®ç¼ºå°‘åˆ—: {missing_cols}")
            
            missing_cols = [col for col in required_cols if col not in target_df.columns]
            if missing_cols:
                raise ValueError(f"ç›®æ ‡æ•°æ®ç¼ºå°‘åˆ—: {missing_cols}")
            
            log(f"ä½¿ç”¨åˆ—å: ID={id_col}, æœŸåˆŠ={journal_col}, åˆ†ç±»={category_col}, å¼•ç”¨={refs_col}")
            
            # é˜¶æ®µ1: æ„å»ºå­¦ç§‘åˆ†ç±»çŸ¥è¯†åº“
            log("\n[é˜¶æ®µ1] æ„å»ºå­¦ç§‘åˆ†ç±»çŸ¥è¯†åº“...")
            paper_categories = {}
            
            for _, row in background_df.iterrows():
                paper_id = str(row[id_col])
                if pd.isna(paper_id):
                    continue
                
                categories = self.parse_categories(row[category_col])
                if categories:
                    paper_categories[paper_id] = categories
            
            log(f"  å¤„ç† {len(paper_categories)} ç¯‡è®ºæ–‡çš„åˆ†ç±»ä¿¡æ¯")
            
            # è®¡ç®—å­¦ç§‘ç›¸ä¼¼æ€§çŸ©é˜µ
            self.calculate_similarity_matrix(paper_categories)
            
            # é˜¶æ®µ2: åˆ†æç›®æ ‡æ•°æ®
            log("\n[é˜¶æ®µ2] åˆ†æç›®æ ‡æœŸåˆŠæ•°æ®...")
            paper_results = []
            
            target_df = target_df.copy()
            target_df['parsed_refs'] = target_df[refs_col].apply(
                lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else []
            )
            
            td_scores = []
            for idx, row in target_df.iterrows():
                paper_id = str(row[id_col])
                journal = row[journal_col]
                refs = row['parsed_refs']
                
                ref_categories = []
                for ref_id in refs:
                    if ref_id in paper_categories:
                        ref_categories.extend(paper_categories[ref_id])
                
                td_score = self.calculate_td_index(ref_categories)
                td_scores.append(td_score)
                
                paper_results.append({
                    'paper_id': paper_id,
                    'journal': journal,
                    'td_score': td_score
                })
                
                if (idx + 1) % 100 == 0:
                    log(f"  å¤„ç†è¿›åº¦: {idx + 1}/{len(target_df)}")
            
            # åˆ›å»ºè®ºæ–‡ç»“æœDataFrame
            paper_df = pd.DataFrame(paper_results)
            
            # è®¡ç®—å½’ä¸€åŒ–ç™¾åˆ†åˆ¶åˆ†æ•°
            log("\n[é˜¶æ®µ3] è®¡ç®—å½’ä¸€åŒ–ç™¾åˆ†åˆ¶åˆ†æ•°...")
            normalized_percent = self.normalize_to_percent(td_scores)
            paper_df['td_score_percent'] = normalized_percent
            
            # æŒ‰æœŸåˆŠèšåˆ
            if 'journal' in paper_df.columns:
                journal_col_name = 'journal'
            elif 'Journal' in paper_df.columns:
                journal_col_name = 'Journal'
            else:
                journal_col_name = paper_df.columns[1]
            
            # èšåˆç»Ÿè®¡
            journal_agg = paper_df.groupby(journal_col_name).agg({
                'td_score': ['mean', 'std'],
                'td_score_percent': ['mean', 'std'],
                'paper_id': 'count'
            }).round(4)
            
            journal_agg.columns = ['TD_Mean', 'TD_Std', 'TD_Percent_Mean', 'TD_Percent_Std', 'Paper_Count']
            journal_agg = journal_agg.sort_values('TD_Percent_Mean', ascending=False).reset_index()
            journal_agg = journal_agg.rename(columns={journal_col_name: 'Journal'})
            
            # ç”Ÿæˆä¸¤ä¸ªæŸ±çŠ¶å›¾å’Œç™¾åˆ†åˆ¶å¾—åˆ†åˆ—è¡¨
            self.generate_outputs(journal_agg)
            
            log("\nâœ… åˆ†æå®Œæˆï¼")
            log(f"ğŸ“Š ç”Ÿæˆæ–‡ä»¶:")
            log(f"  1. journal_td_original.png - åŸå§‹TDå¾—åˆ†æŸ±çŠ¶å›¾")
            log(f"  2. journal_td_percent.png - ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾")
            log(f"  3. journal_percent_scores.csv - ç™¾åˆ†åˆ¶å¾—åˆ†åˆ—è¡¨")
            
        except Exception as e:
            log(f"[é”™è¯¯] åˆ†æè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

    def generate_outputs(self, journal_agg):
        """ç”Ÿæˆä¸¤ä¸ªæŸ±çŠ¶å›¾å’Œç™¾åˆ†åˆ¶å¾—åˆ†åˆ—è¡¨"""
        # åªè¾“å‡ºå‰20ä¸ªæœŸåˆŠï¼ˆæˆ–å…¨éƒ¨ï¼‰
        display_data = journal_agg.head(20) if len(journal_agg) > 20 else journal_agg
        
        # 1. ç”ŸæˆåŸå§‹TDå¾—åˆ†æŸ±çŠ¶å›¾
        self.create_bar_chart(
            data=display_data,
            value_col='TD_Mean',
            title='æœŸåˆŠè·¨å­¦ç§‘æ€§æŒ‡æ•° - åŸå§‹TDå¾—åˆ†',
            xlabel='åŸå§‹TDæŒ‡æ•°',
            filename='journal_td_original.png',
            color='steelblue'
        )
        
        # 2. ç”Ÿæˆç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾
        self.create_bar_chart(
            data=display_data,
            value_col='TD_Percent_Mean',
            title='æœŸåˆŠè·¨å­¦ç§‘æ€§æŒ‡æ•° - ç™¾åˆ†åˆ¶å¾—åˆ†',
            xlabel='ç™¾åˆ†åˆ¶å¾—åˆ† (0-100)',
            filename='journal_td_percent.png',
            color='forestgreen',
            is_percent=True
        )
        
        # 3. ç”Ÿæˆç™¾åˆ†åˆ¶å¾—åˆ†åˆ—è¡¨
        percent_list = journal_agg[['Journal', 'TD_Percent_Mean', 'TD_Mean', 'Paper_Count']].copy()
        percent_list = percent_list.rename(columns={
            'Journal': 'æœŸåˆŠåç§°',
            'TD_Percent_Mean': 'ç™¾åˆ†åˆ¶å¾—åˆ†',
            'TD_Mean': 'åŸå§‹TDå¾—åˆ†',
            'Paper_Count': 'è®ºæ–‡æ•°é‡'
        })
        
        # æ’åºå¹¶ä¿å­˜
        percent_list = percent_list.sort_values('ç™¾åˆ†åˆ¶å¾—åˆ†', ascending=False)
        csv_path = self.output_dir / "journal_percent_scores.csv"
        percent_list.to_csv(csv_path, index=False, encoding="utf-8-sig", float_format='%.2f')
        log(f"ğŸ“„ ç™¾åˆ†åˆ¶å¾—åˆ†åˆ—è¡¨å·²ä¿å­˜: {csv_path}")

    def create_bar_chart(self, data, value_col, title, xlabel, filename, color, is_percent=False):
        """åˆ›å»ºç¾è§‚çš„æŸ±çŠ¶å›¾"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # è®¾ç½®æœŸåˆŠåç§°ï¼ˆç¼©çŸ­é•¿åç§°ï¼‰
        journal_names = []
        for name in data['Journal']:
            if len(str(name)) > 25:
                journal_names.append(str(name)[:22] + '...')
            else:
                journal_names.append(str(name))
        
        # åˆ›å»ºæŸ±çŠ¶å›¾
        bars = ax.barh(range(len(data)), data[value_col], 
                      color=color, alpha=0.8, height=0.7, 
                      edgecolor='white', linewidth=1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, val, count) in enumerate(zip(bars, data[value_col], data['Paper_Count'])):
            # åœ¨æŸ±å­å³ä¾§æ˜¾ç¤ºæ•°å€¼
            if is_percent:
                label_text = f'{val:.1f}åˆ†\n({count}ç¯‡)'
            else:
                label_text = f'{val:.2f}\n({count}ç¯‡)'
            
            ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
                   label_text, ha='left', va='center',
                   fontsize=10, color='black')
        
        # è®¾ç½®yè½´æ ‡ç­¾ï¼ˆæœŸåˆŠåç§°ï¼‰
        ax.set_yticks(range(len(data)))
        ax.set_yticklabels(journal_names, fontsize=10)
        
        # è®¾ç½®xè½´
        ax.set_xlabel(xlabel, fontsize=12, fontweight='bold')
        
        # è®¾ç½®æ ‡é¢˜
        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)
        
        # ç¾åŒ–ç½‘æ ¼
        ax.grid(axis='x', alpha=0.2, linestyle='--')
        
        # åè½¬yè½´ï¼ˆä»é«˜åˆ°ä½ï¼‰
        ax.invert_yaxis()
        
        # å¦‚æœæ˜¾ç¤ºç™¾åˆ†åˆ¶ï¼Œè®¾ç½®xè½´èŒƒå›´ä¸º0-100
        if is_percent:
            ax.set_xlim([0, 105])
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        img_path = self.output_dir / filename
        plt.savefig(img_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        log(f"ğŸ“ˆ æŸ±çŠ¶å›¾å·²ä¿å­˜: {img_path}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        analyzer = InterdisciplinaryAnalyzer()
        results = analyzer.run_analysis()
    except Exception as e:
        print(f"[é”™è¯¯] ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()