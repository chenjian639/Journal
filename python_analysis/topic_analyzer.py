# -*- coding: utf-8 -*-
"""
python_analysis/topic_analyzer.py
è·¨å­¦ç§‘æ€§åˆ†æï¼ˆé¦™å†œç†µæ–¹æ³•ï¼‰- å®Œæ•´ç‰ˆ
ç™¾åˆ†åˆ¶å¾—åˆ† = åŸå§‹ç†µå€¼ Ã— 100 Ã— 5
è¾“å‡ºï¼šå¾—åˆ†åˆ—è¡¨ + ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾
"""
import json
import pandas as pd
import numpy as np
import ast
import re
import difflib
import matplotlib.pyplot as plt
from pathlib import Path
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib.font_manager as fm
try:
    font_path = "C:/Windows/Fonts/simhei.ttf"
    if Path(font_path).exists():
        fm.fontManager.addfont(font_path)
        font_name = fm.FontProperties(fname=font_path).get_name()
        plt.rcParams['font.sans-serif'] = [font_name]
    plt.rcParams['axes.unicode_minus'] = False
except:
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent.parent / 'config.json'
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    return config.get("topic", {})

def log(msg):
    """ç®€å•çš„æ—¥å¿—å‡½æ•°"""
    print(f"[topic] {msg}")

# ============================================================================
#  å®Œæ•´çš„é¢†åŸŸåˆ†ç±»å­—å…¸ï¼ˆä½¿ç”¨å®Œæ•´çš„å­—å…¸ï¼‰
# ============================================================================
FOS_dict = {
    "psychology": [
        "cognition", "cognitive", "executive function", "working memory",
        "memory retrieval", "attention", "selective attention",
        "decision making", "problem solving", "reasoning",
        "mental representation", "information processing",
        "cognitive control", "metacognition", "inhibition",
        "visual perception", "auditory perception", "language processing",
        "skill acquisition", "implicit learning", "explicit learning",
        "concept formation", "judgment", "mental imagery",
        "semantic processing", "episodic memory", "short-term memory",
        "neural", "neural basis", "neural processing", "brain activity",
        "neurocognition", "neuropsychology", "neurobehavioral",
        "prefrontal cortex", "hippocampus", "amygdala", "cortical",
        "neuroplasticity", "brain networks", "neuroimaging",
        "erp", "p300", "n400", "fmri", "eeg", "p600",
        "emotion", "emotional processing", "affect",
        "emotion regulation", "emotional arousal",
        "empathy", "mood", "affective response",
        "emotion recognition", "emotional cognition",
        "motivation", "intrinsic motivation", "extrinsic motivation",
        "goal orientation", "reward processing", "novelty seeking",
        "sensation seeking", "value processing", "self-efficacy",
        "creativity", "creative thinking", "creative cognition",
        "divergent thinking", "convergent thinking", "idea generation",
        "personality", "personality traits", "big five", "neuroticism",
        "extraversion", "openness", "agreeableness", "conscientiousness",
        "behavior", "behavioral response", "behavioral performance",
        "social cognition", "social interaction", "social influence",
        "developmental psychology", "child development",
        "clinical psychology", "mental health", "psychopathology",
        "depression", "anxiety", "stress", "trauma",
        "educational psychology", "learning motivation", "learning strategies",
    ],

    "neuroscience": [
        "brain", "neural", "neuron", "neural networks",
        "central nervous system", "cns", "neuroscience",
        "synaptic", "neuroplasticity", "neural pathway",
        "neural circuit", "neural dynamics", "neurophysiology",
        "dopamine", "serotonin", "norepinephrine", "acetylcholine",
        "glutamate", "gaba", "oxytocin", "vasopressin",
        "prefrontal cortex", "pfc", "orbitofrontal cortex", "ofc",
        "anterior cingulate cortex", "acc", "posterior cingulate cortex",
        "hippocampus", "amygdala", "insula", "basal ganglia",
        "striatum", "cerebellum", "thalamus", "hypothalamus",
        "synaptic plasticity", "long-term potentiation", "ltp",
        "long-term depression", "ltd", "signal transmission",
        "action potential", "spike train", "neural oscillation",
        "working memory", "executive function", "decision making",
        "reward processing", "attention network",
        "emotion regulation", "perception", "sensory processing",
        "eeg", "erp", "p300", "n400", "p600",
        "meg", "fmri", "bold signal", "pet scan",
        "neuroimaging", "diffusion tensor imaging", "dti",
        "ion channel", "synapse", "axon", "dendrite",
        "behavioral neuroscience", "neurobehavioral",
        "fear conditioning", "reinforcement learning",
        "computational model", "spiking model",
        "neural computation", "neural coding",
        "alzheimer", "parkinson", "adhd",
        "autism", "epilepsy", "schizophrenia",
    ],

    "computer_science": [
        "algorithm", "algorithms", "optimization", "approximation",
        "graph algorithm", "graph theory",
        "search algorithm", "sorting", "complexity",
        "data structure", "tree", "graph", "hashing",
        "machine learning", "supervised learning", "unsupervised learning",
        "reinforcement learning", "deep learning",
        "neural network", "neural networks",
        "convolutional neural network", "cnn",
        "recurrent neural network", "rnn", "transformer",
        "representation learning", "feature extraction",
        "classification", "regression", "clustering",
        "data mining", "data analysis", "data processing",
        "big data", "data visualization",
        "natural language processing", "nlp",
        "text mining", "text classification", "sentiment analysis",
        "language model", "word embedding", "transformer model",
        "computer vision", "image processing", "object detection",
        "image classification", "image recognition",
        "human computer interaction", "hci",
        "robotics", "autonomous system", "autonomous agents",
        "software engineering", "software architecture",
        "operating system", "distributed system",
        "parallel computing", "cloud computing",
        "computer network", "network protocol",
        "cybersecurity", "cryptography", "encryption",
        "simulation", "agent-based model",
        "computational model", "numerical simulation",
    ],

    "education": [
        "education", "educational practice", "educational research",
        "learning", "instruction", "teaching", "pedagogy",
        "instructional design", "curriculum design", "learning outcomes",
        "student performance", "academic performance",
        "learning behavior", "classroom environment",
        "learning process", "knowledge acquisition",
        "constructivism", "social constructivism",
        "experiential learning", "active learning",
        "collaborative learning", "problem-based learning",
        "self-directed learning", "self-regulated learning",
        "educational psychology", "motivation", "learning motivation",
        "self-efficacy", "goal orientation", "engagement",
        "assessment", "evaluation", "formative assessment",
        "summative assessment", "rubric", "performance assessment",
        "learning analytics", "measurement", "testing",
        "instructional method", "instructional strategy",
        "scaffolding", "differentiated instruction",
        "educational technology", "technology-enhanced learning",
        "digital learning", "online learning", "blended learning",
        "e-learning", "mobile learning", "virtual learning",
        "higher education", "tertiary education",
        "k-12 education", "primary education", "secondary education",
        "teacher education", "teacher training", "teacher development",
        "curriculum", "curriculum implementation",
        "educational policy", "education reform",
        "creative behavior", "creative learning",
    ],

    "biomedical_sciences": [
        "dopamine", "serotonin", "glutamate", "gaba", "acetylcholine",
        "genetics", "genomics", "epigenetics", "gene expression",
        "gene regulation", "transcription factor", "molecular pathway",
        "protein expression", "protein folding", "protein interaction",
        "biochemical", "biochemical pathway", "biomarker", "cytokine",
        "inflammation", "inflammatory response", "immune system",
        "immunity", "innate immunity", "adaptive immunity",
        "neural basis", "neural circuit", "neurobiological",
        "neurochemical", "neurophysiological", "synaptic plasticity",
        "synapse", "axon", "dendrite", "neural signaling",
        "cellular process", "cell culture",
        "cell proliferation", "cell differentiation", "stem cell",
        "neural stem cell", "neurogenesis",
        "oxidative stress", "mitochondria", "mitochondrial function",
        "apoptosis", "cell death", "autophagy",
        "endocrine", "hormone", "hormonal regulation",
        "cortisol", "testosterone", "estrogen",
        "neurodevelopmental", "developmental biology",
        "neurodegeneration", "neurodegenerative disease",
        "alzheimer's disease", "parkinson's disease",
        "schizophrenia", "depression", "mental disorder",
        "pharmacology", "drug response", "drug metabolism",
        "metabolism", "metabolic pathway", "lipid metabolism",
        "glucose metabolism", "metabolomics",
        "proteomics", "transcriptomics", "multiomics",
        "microbiome", "gut microbiota",
        "immune response", "cell signaling",
        "signal transduction", "receptor activation",
        "blood brain barrier", "neurovascular",
        "cerebral cortex", "hippocampus", "amygdala",
        "in vivo", "in vitro", "animal model",
        "mouse model", "rat model",
        "biostatistics", "epidemiology",
        "public health", "clinical research",
    ],
}

# ============================================================================
#  å…³é”®è¯å¤„ç†å‡½æ•°
# ============================================================================
def clean_author_keywords(keywords_str):
    """æ¸…æ´—ä½œè€…å…³é”®è¯"""
    if pd.isna(keywords_str):
        return []
    
    if isinstance(keywords_str, str):
        # å¤„ç†åˆ—è¡¨æ ¼å¼
        if keywords_str.startswith('[') and keywords_str.endswith(']'):
            try:
                return ast.literal_eval(keywords_str)
            except:
                pass
        
        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼
        keywords = re.split(r'[,;]', keywords_str)
        cleaned_keywords = []
        for kw in keywords:
            kw = kw.strip().lower()
            if kw:
                cleaned_keywords.append(kw)
        return cleaned_keywords
    
    return []

def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"\d+", "", text)
    text = re.sub(r"[^\w\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    
    return text

def extract_keywords_from_text(text, min_length=3, max_keywords=20):
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
    if not text:
        return []
    
    words = text.split()
    
    stop_words = {
        'the', 'and', 'for', 'with', 'that', 'this', 'these', 'those',
        'from', 'have', 'has', 'had', 'were', 'was', 'are', 'is',
        'be', 'been', 'being', 'by', 'in', 'on', 'at', 'to', 'of',
        'a', 'an', 'as', 'or', 'but', 'not', 'it', 'its', 'they',
        'them', 'their', 'we', 'our', 'you', 'your', 'he', 'she',
        'his', 'her', 'its', 'my', 'mine', 'us', 'our', 'ours'
    }
    
    keywords = []
    for word in words:
        word_lower = word.lower()
        if (len(word_lower) >= min_length and 
            word_lower not in stop_words and
            word_lower.isalpha()):
            keywords.append(word_lower)
    
    word_counts = Counter(keywords)
    top_keywords = [word for word, _ in word_counts.most_common(max_keywords)]
    
    return top_keywords

def map_keyword_to_fields(keyword, field_dict):
    """æ˜ å°„å…³é”®è¯åˆ°é¢†åŸŸ"""
    keyword = keyword.lower().strip()
    
    # ç²¾ç¡®åŒ¹é…
    matched_fields = []
    for field, words in field_dict.items():
        if keyword in words:
            matched_fields.append(field)
    
    if matched_fields:
        return list(set(matched_fields))
    
    # æ¨¡ç³ŠåŒ¹é…
    for field, words in field_dict.items():
        for w in words:
            ratio = difflib.SequenceMatcher(None, keyword, w).ratio()
            if ratio >= 0.75:  # é™ä½é˜ˆå€¼ä»¥æé«˜åŒ¹é…ç‡
                matched_fields.append(field)
                break
    
    return list(set(matched_fields))

def field_distribution(keyword_list, field_dict):
    """è®¡ç®—é¢†åŸŸåˆ†å¸ƒ"""
    field_counter = Counter()

    for kw in keyword_list:
        fields = map_keyword_to_fields(kw, field_dict)
        for f in fields:
            field_counter[f] += 1
    
    total = sum(field_counter.values())
    if total == 0:
        return {}, {}
    
    counts = dict(field_counter)
    shares = {f: count/total for f, count in counts.items()}
    
    return counts, shares

def calculate_shannon_entropy(shares_dict):
    """è®¡ç®—é¦™å†œç†µ"""
    if not shares_dict:
        return 0.0
    
    ps = np.array([p for p in shares_dict.values() if p > 0.0], dtype=float)
    if ps.size == 0:
        return 0.0
    
    ps = ps / ps.sum()
    entropy = -np.sum(ps * np.log2(ps))
    
    return float(entropy)

def calculate_percent_score(entropy):
    """è®¡ç®—ç™¾åˆ†åˆ¶å¾—åˆ†ï¼šåŸå§‹ç†µå€¼ Ã— 100 Ã— 5"""
    percent_score = entropy * 100 * 5
    return round(percent_score, 1)

# ============================================================================
#  ä¸»åˆ†æå™¨ç±»
# ============================================================================
class InterdisciplinaryEntropyAnalyzer:
    def __init__(self, config=None):
        self.config = config or load_config()
        
        # ä»é…ç½®è·å–æœ¯è¯­æ¥æºè®¾ç½®
        self.term_source = self.config.get('parameters', {}).get('term_source', 'keywords')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir_key = 'topic_dir'
        if output_dir_key not in self.config.get('output', {}):
            output_dir_key = 'topic_dir'
        
        output_dir = Path(self.config['output'][output_dir_key])
        output_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir = output_dir
        
        log(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        log(f"æœ¯è¯­æ¥æº: {self.term_source}")

    def extract_terms_from_paper(self, row):
        """ä»å•ç¯‡è®ºæ–‡ä¸­æå–æœ¯è¯­"""
        terms = []
        
        # ä»å…³é”®è¯æå–
        if self.term_source in ['keywords', 'both']:
            keywords_col = self.config['columns']['keywords'] if 'keywords' in self.config['columns'] else 'Keywords'
            if keywords_col in row:
                author_keywords = clean_author_keywords(row[keywords_col])
                terms.extend(author_keywords)
        
        # ä»æ‘˜è¦æå–
        if self.term_source in ['abstract', 'both']:
            if 'Abstract' in row:
                clean_abstract = clean_text(row['Abstract'])
                abstract_keywords = extract_keywords_from_text(clean_abstract, max_keywords=15)
                terms.extend(abstract_keywords)
        
        # å»é‡å¹¶è¿”å›
        unique_terms = list(set(terms))
        return unique_terms

    def run_analysis(self):
        """è¿è¡Œåˆ†æ"""
        try:
            log("=" * 50)
            log("å¼€å§‹æœŸåˆŠè·¨å­¦ç§‘æ€§åˆ†æï¼ˆé¦™å†œç†µæ–¹æ³•ï¼‰")
            log("=" * 50)
            
            # åŠ è½½æ•°æ®
            project_root = Path(__file__).parent.parent
            target_path = project_root / self.config['data_sources']['target_data']
            
            log(f"åŠ è½½æ•°æ®: {target_path}")
            target_df = pd.read_csv(target_path)
            log(f"æ•°æ®å½¢çŠ¶: {target_df.shape}")
            
            # è·å–åˆ—å
            id_col = self.config['columns']['id']
            journal_col = self.config['columns']['journal']
            
            log(f"ä½¿ç”¨åˆ—: ID={id_col}, æœŸåˆŠ={journal_col}")
            
            # è®¡ç®—æ¯ç¯‡è®ºæ–‡çš„ç†µå€¼
            log("\nğŸ“Š è®¡ç®—è®ºæ–‡ç†µå€¼...")
            results = []
            
            total_papers = len(target_df)
            for idx, row in target_df.iterrows():
                paper_id = str(row[id_col]) if pd.notna(row.get(id_col)) else f"paper_{idx}"
                journal = row[journal_col] if pd.notna(row.get(journal_col)) else "Unknown"
                
                # æå–æœ¯è¯­
                terms = self.extract_terms_from_paper(row)
                
                # è®¡ç®—é¢†åŸŸåˆ†å¸ƒ
                field_counts, field_shares = field_distribution(terms, FOS_dict)
                
                # è®¡ç®—é¦™å†œç†µ
                entropy = calculate_shannon_entropy(field_shares)
                
                results.append({
                    'paper_id': paper_id,
                    'journal': journal,
                    'entropy': entropy,
                    'field_count': len(field_counts),
                    'term_count': len(terms)
                })
                
                # è¿›åº¦æ˜¾ç¤º
                if (idx + 1) % 100 == 0 or (idx + 1) == total_papers:
                    progress = (idx + 1) / total_papers * 100
                    log(f"è¿›åº¦: {idx + 1}/{total_papers} ({progress:.1f}%)")
            
            paper_df = pd.DataFrame(results)
            log(f"è®ºæ–‡è®¡ç®—å®Œæˆï¼Œå…± {len(paper_df)} ç¯‡è®ºæ–‡")
            log(f"å¹³å‡æ¯ç¯‡è®ºæ–‡æœ¯è¯­æ•°: {paper_df['term_count'].mean():.1f}")
            log(f"å¹³å‡æ¯ç¯‡è®ºæ–‡é¢†åŸŸæ•°: {paper_df['field_count'].mean():.1f}")
            
            # æŒ‰æœŸåˆŠèšåˆ
            log("\nğŸ“ˆ æŒ‰æœŸåˆŠèšåˆ...")
            journal_agg = paper_df.groupby('journal').agg({
                'entropy': 'mean',
                'field_count': 'mean',
                'paper_id': 'count'
            }).reset_index()
            
            journal_agg.columns = ['æœŸåˆŠåç§°', 'åŸå§‹ç†µå€¼', 'å¹³å‡é¢†åŸŸæ•°', 'è®ºæ–‡æ•°é‡']
            
            # è®¡ç®—ç™¾åˆ†åˆ¶å¾—åˆ†ï¼šåŸå§‹ç†µå€¼ Ã— 100 Ã— 5
            log("ğŸ¯ è®¡ç®—ç™¾åˆ†åˆ¶å¾—åˆ†...")
            journal_agg['ç™¾åˆ†åˆ¶å¾—åˆ†'] = journal_agg['åŸå§‹ç†µå€¼'].apply(calculate_percent_score)
            
            # æŒ‰ç™¾åˆ†åˆ¶å¾—åˆ†æ’åº
            journal_agg = journal_agg.sort_values('ç™¾åˆ†åˆ¶å¾—åˆ†', ascending=False).reset_index(drop=True)
            
            # æ·»åŠ æ’å
            journal_agg.insert(0, 'æ’å', range(1, len(journal_agg) + 1))
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            log(f"\nğŸ“Š å¾—åˆ†ç»Ÿè®¡:")
            log(f"  åŸå§‹ç†µå€¼èŒƒå›´: {journal_agg['åŸå§‹ç†µå€¼'].min():.4f} - {journal_agg['åŸå§‹ç†µå€¼'].max():.4f}")
            log(f"  åŸå§‹ç†µå€¼å‡å€¼: {journal_agg['åŸå§‹ç†µå€¼'].mean():.4f}")
            log(f"  ç™¾åˆ†åˆ¶å¾—åˆ†èŒƒå›´: {journal_agg['ç™¾åˆ†åˆ¶å¾—åˆ†'].min():.1f} - {journal_agg['ç™¾åˆ†åˆ¶å¾—åˆ†'].max():.1f}")
            log(f"  ç™¾åˆ†åˆ¶å¾—åˆ†å‡å€¼: {journal_agg['ç™¾åˆ†åˆ¶å¾—åˆ†'].mean():.1f}")
            log(f"  æ€»æœŸåˆŠæ•°: {len(journal_agg)}")
            
            # è¾“å‡ºæ–‡ä»¶
            self.generate_outputs(journal_agg)
            
            log("\nâœ… åˆ†æå®Œæˆï¼")
            log(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
            log(f"  1. journal_entropy_scores.csv - æœŸåˆŠç†µå€¼å¾—åˆ†åˆ—è¡¨")
            log(f"  2. journal_entropy_percent_chart.png - ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾")
            
            return journal_agg
            
        except Exception as e:
            log(f"é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    def generate_outputs(self, journal_data):
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶"""
        # 1. ä¿å­˜CSVæ–‡ä»¶
        csv_path = self.output_dir / "journal_entropy_scores.csv"
        
        # æ ¼å¼åŒ–è¾“å‡º
        output_df = journal_data.copy()
        output_df['åŸå§‹ç†µå€¼'] = output_df['åŸå§‹ç†µå€¼'].round(4)
        output_df['å¹³å‡é¢†åŸŸæ•°'] = output_df['å¹³å‡é¢†åŸŸæ•°'].round(2)
        output_df['ç™¾åˆ†åˆ¶å¾—åˆ†'] = output_df['ç™¾åˆ†åˆ¶å¾—åˆ†'].round(1)
        
        output_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        log(f"ğŸ“„ å¾—åˆ†åˆ—è¡¨å·²ä¿å­˜: {csv_path}")
        
        # 2. ç”Ÿæˆç¾è§‚çš„æŸ±çŠ¶å›¾
        self.create_beautiful_chart(journal_data)

    def create_beautiful_chart(self, data):
        """åˆ›å»ºç¾è§‚çš„ç™¾åˆ†åˆ¶å¾—åˆ†æŸ±çŠ¶å›¾"""
        if len(data) == 0:
            log("æ²¡æœ‰æ•°æ®å¯ç”Ÿæˆå›¾è¡¨")
            return
        
        # è®¾ç½®å›¾è¡¨å°ºå¯¸
        n_journals = len(data)
        fig_height = max(6, n_journals * 0.35)
        fig, ax = plt.subplots(figsize=(14, fig_height))
        
        # ä½¿ç”¨æ¸å˜è‰²
        colors = plt.cm.plasma(np.linspace(0.2, 0.9, n_journals))
        
        # åˆ›å»ºæ°´å¹³æŸ±çŠ¶å›¾
        y_positions = np.arange(n_journals)
        bars = ax.barh(y_positions, data['ç™¾åˆ†åˆ¶å¾—åˆ†'], 
                      color=colors, 
                      alpha=0.85,
                      height=0.7,
                      edgecolor='white',
                      linewidth=1.5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, percent_score, journal_name, paper_count, raw_score) in enumerate(
            zip(bars, data['ç™¾åˆ†åˆ¶å¾—åˆ†'], data['æœŸåˆŠåç§°'], data['è®ºæ–‡æ•°é‡'], data['åŸå§‹ç†µå€¼'])):
            
            # ç™¾åˆ†åˆ¶å¾—åˆ†æ ‡ç­¾ï¼ˆå³ä¾§ï¼‰
            label_x = bar.get_width() + max(data['ç™¾åˆ†åˆ¶å¾—åˆ†']) * 0.02
            ax.text(label_x, bar.get_y() + bar.get_height()/2,
                   f'{percent_score:.1f}åˆ†', 
                   ha='left', va='center',
                   fontsize=10, fontweight='bold',
                   color='#2c3e50',
                   bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))
            
            # åœ¨æŸ±å­å·¦ä¾§æ˜¾ç¤ºæœŸåˆŠåç§°å’Œæ’å
            short_name = journal_name[:28] + "..." if len(journal_name) > 28 else journal_name
            rank_text = f"{i+1}. {short_name}"
            ax.text(-max(data['ç™¾åˆ†åˆ¶å¾—åˆ†']) * 0.02, bar.get_y() + bar.get_height()/2,
                   rank_text,
                   ha='right', va='center',
                   fontsize=9.5, fontweight='medium',
                   color='#34495e')
            
            # åœ¨æŸ±å­å†…éƒ¨æ˜¾ç¤ºè®ºæ–‡æ•°é‡å’ŒåŸå§‹ç†µå€¼
            if bar.get_width() > max(data['ç™¾åˆ†åˆ¶å¾—åˆ†']) * 0.1:
                inner_text = f"{paper_count}ç¯‡\n{raw_score:.3f}"
                ax.text(bar.get_width()/2, bar.get_y() + bar.get_height()/2,
                       inner_text,
                       ha='center', va='center',
                       fontsize=8, color='white',
                       fontweight='bold')
        
        # è®¾ç½®yè½´
        ax.set_yticks(y_positions)
        ax.set_yticklabels([])
        
        # è®¾ç½®xè½´
        ax.set_xlabel('ç™¾åˆ†åˆ¶å¾—åˆ† ', fontsize=11, fontweight='bold', color='#2c3e50')
        
        # åŠ¨æ€è®¾ç½®xè½´èŒƒå›´
        max_score = max(data['ç™¾åˆ†åˆ¶å¾—åˆ†'])
        ax.set_xlim([0, max_score * 1.15])
        
        # è®¾ç½®æ ‡é¢˜
        title_source = {
            'keywords': 'å…³é”®è¯',
            'abstract': 'æ‘˜è¦',
            'both': 'å…³é”®è¯+æ‘˜è¦'
        }.get(self.term_source, self.term_source)
        
        ax.set_title(f'æœŸåˆŠå¤æ‚åº¦æ’å - åŸºäº{title_source}çš„é¦™å†œç†µåˆ†æ', 
                    fontsize=14, fontweight='bold', pad=20, color='#2c3e50')
        
        # ç¾åŒ–æ ·å¼
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['left'].set_visible(False)
        ax.spines['bottom'].set_color('#95a5a6')
        
        # ç½‘æ ¼çº¿
        ax.grid(axis='x', linestyle='--', alpha=0.25, color='#bdc3c7')
        
        # åè½¬yè½´ï¼ˆä»é«˜åˆ°ä½ï¼‰
        ax.invert_yaxis()
        
        # èƒŒæ™¯è‰²
        ax.set_facecolor('#f8f9fa')
        fig.patch.set_facecolor('white')
        
        # åˆ»åº¦çº¿
        ax.tick_params(axis='x', colors='#7f8c8d')
        ax.tick_params(axis='y', length=0)
        
        # æ·»åŠ å…¬å¼è¯´æ˜
        formula_text = "å¾—åˆ†å…¬å¼: ç™¾åˆ†åˆ¶å¾—åˆ† = åŸå§‹ç†µå€¼ Ã— 100"
        ax.text(0.5, -0.05, formula_text,
               transform=ax.transAxes,
               ha='center', va='center',
               fontsize=9, style='italic',
               color='#7f8c8d')
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        plt.subplots_adjust(bottom=0.12)
        
        # ä¿å­˜å›¾ç‰‡
        img_path = self.output_dir / "journal_entropy_percent_chart.png"
        plt.savefig(img_path, dpi=300, bbox_inches='tight', 
                    facecolor=fig.get_facecolor(), edgecolor='none')
        plt.close()
        
        log(f"ğŸ“Š æŸ±çŠ¶å›¾å·²ä¿å­˜: {img_path}")

# ============================================================================
#  ä¸»å‡½æ•°
# ============================================================================
def main():
    try:
        analyzer = InterdisciplinaryEntropyAnalyzer()
        results = analyzer.run_analysis()
        
        # æ˜¾ç¤ºç»“æœ
        if results is not None and len(results) > 0:
            print("\n" + "="*80)
            print("æœŸåˆŠè·¨å­¦ç§‘æ€§å¾—åˆ†ï¼‰")
            print("="*80)
            print(f"{'æ’å':^4} | {'æœŸåˆŠåç§°':^40} | {'ç™¾åˆ†åˆ¶å¾—åˆ†':^12} | {'åŸå§‹ç†µå€¼':^10} | {'è®ºæ–‡æ•°':^8}")
            print("-"*80)
            
            for i, row in results.iterrows():
                journal_name = row['æœŸåˆŠåç§°']
                if len(journal_name) > 38:
                    journal_name = journal_name[:35] + "..."
                
                print(f"{row['æ’å']:^4} | {journal_name:^40} | {row['ç™¾åˆ†åˆ¶å¾—åˆ†']:^12.1f} | "
                      f"{row['åŸå§‹ç†µå€¼']:^10.4f} | {row['è®ºæ–‡æ•°é‡']:^8}")
            
            print("="*80)
            
            # æ˜¾ç¤ºå‰5åè¯¦ç»†ä¿¡æ¯
            print(f"\nğŸ† Top 5 æœŸåˆŠè¯¦æƒ…:")
            for i, row in results.head(5).iterrows():
                print(f"{row['æ’å']}. {row['æœŸåˆŠåç§°']}")
                print(f"   ç™¾åˆ†åˆ¶å¾—åˆ†: {row['ç™¾åˆ†åˆ¶å¾—åˆ†']:.1f} | åŸå§‹ç†µå€¼: {row['åŸå§‹ç†µå€¼']:.4f} | "
                      f"è®ºæ–‡æ•°: {row['è®ºæ–‡æ•°é‡']} | å¹³å‡é¢†åŸŸæ•°: {row['å¹³å‡é¢†åŸŸæ•°']:.2f}")
                print()
            
    except Exception as e:
        print(f"ç¨‹åºé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()