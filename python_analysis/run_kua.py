# python_analysis/kua_interdisciplinary.py
"""
è·¨å­¦ç§‘æ€§(TD)è®¡ç®—æ¨¡å—
ç”¨äºè®¡ç®—Top10æœŸåˆŠçš„è·¨å­¦ç§‘æ€§æŒ‡æ ‡
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import ast
from typing import Tuple, List, Dict


class InterdisciplinaryAnalyzer:
    """è·¨å­¦ç§‘æ€§åˆ†æå™¨"""
    
    def __init__(self, root_dir: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            root_dir: é¡¹ç›®æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•çš„ä¸Šä¸€çº§
        """
        if root_dir is None:
            # å‡è®¾srcç›®å½•åœ¨æ ¹ç›®å½•ä¸‹
            self.root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        else:
            self.root_dir = root_dir
        
        # è®¾ç½®è·¯å¾„
        self.data_dir = os.path.join(self.root_dir, 'data', 'raw')
        self.output_dir = os.path.join(self.root_dir, 'outputs', 'kua')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®
        self.df = None
        self.top10_data = None
        self.df_top10 = None
        self.doi_to_category_map = {}
        
        print(f"æ ¹ç›®å½•: {self.root_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_data(self) -> bool:
        """
        åŠ è½½æ•°æ®æ–‡ä»¶
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            # åŠ è½½æ•°æ®
            df_path = os.path.join(self.data_dir, 'data_with_citing.csv')
            top10_path = os.path.join(self.data_dir, 'top10_journals_data.csv')
            
            if not os.path.exists(df_path) or not os.path.exists(top10_path):
                print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                print(f"  - data_with_citing.csv: {os.path.exists(df_path)}")
                print(f"  - top10_journals_data.csv: {os.path.exists(top10_path)}")
                return False
            
            self.df = pd.read_csv(df_path)
            self.top10_data = pd.read_csv(top10_path)
            
            print(f"æ•°æ®åŠ è½½å®Œæˆ")
            print(f"  - å…¨é‡æ•°æ®: {self.df.shape}")
            print(f"  - Top10æ•°æ®: {self.top10_data.shape}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        # å¤„ç†ç©ºå€¼
        self.df['citing'] = self.df['citing'].fillna('')
        
        def safe_literal_eval(x):
            if pd.isna(x) or x == '' or x == '[]':
                return []
            try:
                return ast.literal_eval(x)
            except:
                if isinstance(x, str):
                    x_clean = x.strip('[]').replace("'", "").replace('"', '')
                    items = [item.strip() for item in x_clean.split(',') if item.strip()]
                    return items
                return []
        
        self.df['citing'] = self.df['citing'].apply(safe_literal_eval)
        
        # è·å–Top10æœŸåˆŠåˆ—è¡¨å¹¶è¿‡æ»¤æ•°æ®
        top10_journals = self.top10_data['Source Title'].unique().tolist()
        self.df_top10 = self.df[self.df['Source Title'].isin(top10_journals)].copy()
        
        print(f"æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"  - è¿‡æ»¤åè®ºæ–‡æ•°: {self.df_top10.shape[0]}")
        print(f"  - Top10æœŸåˆŠæ•°: {len(top10_journals)}")
    
    def build_category_mapping(self):
        """å»ºç«‹DOIåˆ°å­¦ç§‘çš„æ˜ å°„"""
        for _, row in self.top10_data.iterrows():
            doi = str(row.get('DOI', '')).strip()
            category = row.get('WoS Categories', '')
            
            if doi and doi.lower() != 'nan' and pd.notna(category):
                # æ ‡å‡†åŒ–DOIæ ¼å¼
                if doi.startswith('https://doi.org/'):
                    doi = doi.replace('https://doi.org/', '')
                elif doi.startswith('http://doi.org/'):
                    doi = doi.replace('http://doi.org/', '')
                elif doi.startswith('doi:'):
                    doi = doi.replace('doi:', '')
                
                self.doi_to_category_map[doi] = str(category).strip()
        
        print(f"ğŸ—ºï¸  å­¦ç§‘æ˜ å°„å»ºç«‹å®Œæˆ: {len(self.doi_to_category_map)}ä¸ª")
    
    def doi_to_categories(self, doi: str) -> List[str]:
        """æ ¹æ®DOIè¿”å›å­¦ç§‘åˆ—è¡¨"""
        if not doi or pd.isna(doi):
            return []
        
        doi_str = str(doi).strip()
        # æ ‡å‡†åŒ–DOIæ ¼å¼
        if doi_str.startswith('https://doi.org/'):
            doi_str = doi_str.replace('https://doi.org/', '')
        elif doi_str.startswith('http://doi.org/'):
            doi_str = doi_str.replace('http://doi.org/', '')
        elif doi_str.startswith('doi:'):
            doi_str = doi_str.replace('doi:', '')
        
        category = self.doi_to_category_map.get(doi_str)
        return [category] if category else []
    
    def get_reference_categories_with_frequency(self, doi_list: List[str]) -> List[str]:
        """è·å–åŒ…å«é¢‘ç‡çš„å­¦ç§‘åˆ—è¡¨"""
        if not doi_list:
            return []
        
        all_categories = []
        for doi in doi_list:
            categories = self.doi_to_categories(doi)
            all_categories.extend(categories)
        
        return all_categories
    
    def get_reference_categories(self, doi_list: List[str]) -> List[str]:
        """è·å–å»é‡çš„å­¦ç§‘åˆ—è¡¨"""
        if not doi_list:
            return []
        
        all_categories = []
        for doi in doi_list:
            categories = self.doi_to_categories(doi)
            all_categories.extend(categories)
        
        return list(set(all_categories))
    
    def build_co_occurrence_matrix(self, df: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
        """æ„å»ºå­¦ç§‘å…±ç°çŸ©é˜µ"""
        all_categories = set()
        for doi_list in df['citing']:
            categories = self.get_reference_categories(doi_list)
            all_categories.update(categories)
        
        all_categories = sorted(list(all_categories))
        n_categories = len(all_categories)
        category_index = {cat: idx for idx, cat in enumerate(all_categories)}
        
        co_occurrence = np.zeros((n_categories, n_categories))
        
        for doi_list in df['citing']:
            categories = self.get_reference_categories(doi_list)
            for i in range(len(categories)):
                idx_i = category_index[categories[i]]
                co_occurrence[idx_i, idx_i] += 1
                
                for j in range(i + 1, len(categories)):
                    idx_j = category_index[categories[j]]
                    co_occurrence[idx_i, idx_j] += 1
                    co_occurrence[idx_j, idx_i] += 1
        
        return co_occurrence, all_categories
    
    def calculate_salton_similarity(self, co_occurrence_matrix: np.ndarray) -> np.ndarray:
        """è®¡ç®—Saltonä½™å¼¦ç›¸ä¼¼æ€§çŸ©é˜µ"""
        n = co_occurrence_matrix.shape[0]
        similarity_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                if i == j:
                    similarity_matrix[i, j] = 1.0
                else:
                    numerator = 0.0
                    for k in range(n):
                        numerator += co_occurrence_matrix[i, k] * co_occurrence_matrix[j, k]
                    
                    denominator_i = np.sqrt(np.sum(co_occurrence_matrix[i, :]**2))
                    denominator_j = np.sqrt(np.sum(co_occurrence_matrix[j, :]**2))
                    denominator = denominator_i * denominator_j
                    
                    if denominator > 0:
                        similarity_matrix[i, j] = numerator / denominator
                    else:
                        similarity_matrix[i, j] = 0.0
        
        return similarity_matrix
    
    def calculate_td_for_paper(self, paper_categories: List[str], 
                              similarity_matrix: np.ndarray, 
                              all_categories: List[str]) -> float:
        """è®¡ç®—å•ç¯‡è®ºæ–‡çš„TDæŒ‡æ ‡"""
        if not paper_categories:
            return 1.0
        
        category_index = {cat: idx for idx, cat in enumerate(all_categories)}
        
        # è®¡ç®—å­¦ç§‘åˆ†å¸ƒ
        category_counts = {}
        for cat in paper_categories:
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        total_refs = len(paper_categories)
        p_vector = np.zeros(len(all_categories))
        
        for cat, count in category_counts.items():
            if cat in category_index:
                idx = category_index[cat]
                p_vector[idx] = count / total_refs
        
        # è®¡ç®—TDæŒ‡æ ‡
        sum_term = 0.0
        for i in range(len(all_categories)):
            for j in range(len(all_categories)):
                sum_term += similarity_matrix[i, j] * p_vector[i] * p_vector[j]
        
        if sum_term > 0:
            return 1.0 / sum_term
        else:
            return 1.0
    
    def calculate_interdisciplinarity(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, List[str], pd.Series]:
        """è®¡ç®—è·¨å­¦ç§‘æ€§æŒ‡æ ‡"""
        print("å¼€å§‹è®¡ç®—è·¨å­¦ç§‘æ€§æŒ‡æ ‡...")
        
        # æ„å»ºå­¦ç§‘å…±ç°çŸ©é˜µ
        print("  1. æ„å»ºå­¦ç§‘å…±ç°çŸ©é˜µ...")
        co_occurrence, all_categories = self.build_co_occurrence_matrix(df)
        print(f"     å­¦ç§‘æ•°: {len(all_categories)}")
        
        # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
        print("  2. è®¡ç®—å­¦ç§‘ç›¸ä¼¼æ€§çŸ©é˜µ...")
        similarity_matrix = self.calculate_salton_similarity(co_occurrence)
        
        # è®¡ç®—æ¯ç¯‡è®ºæ–‡çš„TDæŒ‡æ ‡
        print("  3. è®¡ç®—å•ç¯‡è®ºæ–‡TDæŒ‡æ ‡...")
        td_scores = []
        
        for idx, row in df.iterrows():
            paper_categories = self.get_reference_categories_with_frequency(row['citing'])
            td_score = self.calculate_td_for_paper(paper_categories, similarity_matrix, all_categories)
            td_scores.append(td_score)
        
        df_result = df.copy()
        df_result['TD_Score'] = td_scores
        
        # è®¡ç®—æœŸåˆŠå±‚é¢çš„å¹³å‡TD
        journal_td = df_result.groupby('Source Title')['TD_Score'].mean().sort_values(ascending=False)
        
        print("è·¨å­¦ç§‘æ€§è®¡ç®—å®Œæˆ")
        return df_result, similarity_matrix, all_categories, journal_td
    
    def save_results(self, df_with_td: pd.DataFrame, 
                     similarity_matrix: np.ndarray, 
                     all_categories: List[str], 
                     journal_td: pd.Series):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        print(" ä¿å­˜ç»“æœæ–‡ä»¶...")
        
        # 1. ä¿å­˜æœŸåˆŠTDå¾—åˆ†
        journal_td_df = journal_td.reset_index()
        journal_td_df.columns = ['Journal', 'TD_Score']
        journal_td_df['TD_Score'] = journal_td_df['TD_Score'].round(4)
        
        journal_output_path = os.path.join(self.output_dir, 'journal_td_scores.csv')
        journal_td_df.to_csv(journal_output_path, index=False, encoding='utf-8-sig')
        print(f"æœŸåˆŠTDå¾—åˆ†: {journal_output_path}")
        
        # 2. ä¿å­˜æ¯ç¯‡è®ºæ–‡çš„TDå¾—åˆ†
        paper_output_path = os.path.join(self.output_dir, 'paper_td_scores.csv')
        paper_results = df_with_td[['Source Title', 'DOI', 'TD_Score']].copy()
        paper_results['TD_Score'] = paper_results['TD_Score'].round(4)
        paper_results.to_csv(paper_output_path, index=False, encoding='utf-8-sig')
        print(f"è®ºæ–‡TDå¾—åˆ†: {paper_output_path}")
        
        # 3. ä¿å­˜å­¦ç§‘ç›¸ä¼¼æ€§çŸ©é˜µ
        similarity_output_path = os.path.join(self.output_dir, 'similarity_matrix.csv')
        similarity_df = pd.DataFrame(similarity_matrix, index=all_categories, columns=all_categories)
        similarity_df.to_csv(similarity_output_path, encoding='utf-8-sig')
        print(f"å­¦ç§‘ç›¸ä¼¼æ€§çŸ©é˜µ: {similarity_output_path}")
        
        # 4. ä¿å­˜å­¦ç§‘åˆ—è¡¨
        categories_output_path = os.path.join(self.output_dir, 'categories_list.csv')
        categories_df = pd.DataFrame({'Category': all_categories})
        categories_df.to_csv(categories_output_path, index=False, encoding='utf-8-sig')
        print(f"å­¦ç§‘åˆ—è¡¨: {categories_output_path}")
        
        # 5. ä¿å­˜å¯è§†åŒ–å›¾è¡¨
        self.plot_journal_td(journal_td)
    
    def plot_journal_td(self, journal_td: pd.Series):
        """ç»˜åˆ¶æœŸåˆŠTDå›¾è¡¨å¹¶ä¿å­˜"""
        plt.figure(figsize=(14, 7))
        bars = plt.bar(journal_td.index, journal_td.values)
        plt.title('Journal Interdisciplinarity (TD Index) - Top 10 Journals', fontsize=14, fontweight='bold')
        plt.xlabel('Journal', fontsize=12)
        plt.ylabel('TD Index', fontsize=12)
        plt.xticks(rotation=45, ha='right', fontsize=10)
        
        # åœ¨æ¯ä¸ªæŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_output_path = os.path.join(self.output_dir, 'journal_td_chart.png')
        plt.savefig(chart_output_path, dpi=300, bbox_inches='tight')
        print(f"å¯è§†åŒ–å›¾è¡¨: {chart_output_path}")
        
        plt.show()
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        print("è·¨å­¦ç§‘æ€§(TD)åˆ†æå¼€å§‹")
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_data():
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œåˆ†æç»ˆæ­¢")
            return
        
        # 2. æ•°æ®é¢„å¤„ç†
        self.preprocess_data()
        
        # 3. å»ºç«‹å­¦ç§‘æ˜ å°„
        self.build_category_mapping()
        
        # 4. è®¡ç®—è·¨å­¦ç§‘æ€§
        df_with_td, similarity_matrix, all_categories, journal_td = self.calculate_interdisciplinarity(self.df_top10)
               
        # 6. ä¿å­˜ç»“æœ
        self.save_results(df_with_td, similarity_matrix, all_categories, journal_td)
        print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    analyzer = InterdisciplinaryAnalyzer()
    analyzer.run_analysis()


if __name__ == "__main__":
    main()