# scripts/data_collection/arxiv_collector.py
import requests
import pandas as pd
import xml.etree.ElementTree as ET
from datetime import datetime
import time
import os

def fetch_arxiv_papers(keywords, max_results=100):
    """ä»arXiv APIè·å–è®ºæ–‡æ•°æ®"""
    
    papers_data = []
    
    for keyword in keywords:
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
        
        url = "http://export.arxiv.org/api/query"
        params = {
            'search_query': f'all:{keyword}',
            'start': 0,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            # è§£æXMLå“åº”
            root = ET.fromstring(response.content)
            
            # arXivä½¿ç”¨Atomæ ¼å¼ï¼Œå‘½åç©ºé—´
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            
            for entry in root.findall('atom:entry', ns):
                paper = {}
                
                # æå–è®ºæ–‡ä¿¡æ¯
                paper['id'] = entry.find('atom:id', ns).text.split('/')[-1] if entry.find('atom:id', ns) is not None else ''
                paper['title'] = entry.find('atom:title', ns).text.strip() if entry.find('atom:title', ns) is not None else ''
                paper['summary'] = entry.find('atom:summary', ns).text.strip() if entry.find('atom:summary', ns) is not None else ''
                paper['published'] = entry.find('atom:published', ns).text if entry.find('atom:published', ns) is not None else ''
                paper['updated'] = entry.find('atom:updated', ns).text if entry.find('atom:updated', ns) is not None else ''
                
                # æå–ä½œè€…
                authors = []
                for author in entry.findall('atom:author', ns):
                    name = author.find('atom:name', ns).text if author.find('atom:name', ns) is not None else ''
                    authors.append(name)
                paper['authors'] = ', '.join(authors)
                
                # æå–åˆ†ç±»
                categories = []
                for category in entry.findall('atom:category', ns):
                    cat = category.get('term', '')
                    categories.append(cat)
                paper['categories'] = ', '.join(categories)
                
                # æå–æœŸåˆŠä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                paper['journal_ref'] = entry.find('atom:journal_ref', ns).text if entry.find('atom:journal_ref', ns) is not None else 'arXiv'
                
                papers_data.append(paper)
            
            print(f"âœ… è·å–åˆ° {len(papers_data)} ç¯‡è®ºæ–‡")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
            
        except Exception as e:
            print(f"âŒ è·å– {keyword} æ—¶å‡ºé”™: {e}")
            continue
    
    return papers_data

def save_arxiv_data(papers_data, output_file):
    """ä¿å­˜arXivæ•°æ®åˆ°CSV"""
    df = pd.DataFrame(papers_data)
    
    # æ•°æ®æ¸…æ´—å’Œå¤„ç†
    df['year'] = pd.to_datetime(df['published']).dt.year
    df['published_date'] = pd.to_datetime(df['published'])
    
    # é‡å‘½ååˆ—ä»¥åŒ¹é…æˆ‘ä»¬çš„åˆ†æç³»ç»Ÿ
    df = df.rename(columns={
        'summary': 'abstract',
        'journal_ref': 'journal'
    })
    
    # é€‰æ‹©éœ€è¦çš„åˆ—
    final_columns = ['id', 'title', 'abstract', 'authors', 'journal', 'year', 'published_date', 'categories']
    df = df[[col for col in final_columns if col in df.columns]]
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    df.to_csv(output_file, index=False, encoding='utf-8')
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    return df

def main():
    """ä¸»å‡½æ•°"""
    # æœç´¢å…³é”®è¯ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
    keywords = [
        "machine learning",
        "deep learning", 
        "artificial intelligence",
        "computer vision",
        "natural language processing"
    ]
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = "data/raw/arxiv_ai_papers.csv"
    
    print("ğŸš€ å¼€å§‹ä»arXivæ”¶é›†AIé¢†åŸŸè®ºæ–‡...")
    
    # è·å–æ•°æ®
    papers_data = fetch_arxiv_papers(keywords, max_results=50)  # æ¯ä¸ªå…³é”®è¯50ç¯‡
    
    if papers_data:
        # ä¿å­˜æ•°æ®
        df = save_arxiv_data(papers_data, output_file)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æ€»è®ºæ–‡æ•°: {len(df)}")
        print(f"   æ—¶é—´èŒƒå›´: {df['year'].min()} - {df['year'].max()}")
        print(f"   æœŸåˆŠåˆ†å¸ƒ: {df['journal'].nunique()} ç§æ¥æº")
        print(f"   é¢†åŸŸåˆ†å¸ƒ: {df['categories'].str.split(',').explode().str.strip().nunique()} ä¸ªåˆ†ç±»")
        
        print(f"\nğŸ“ å‰3ç¯‡è®ºæ–‡ç¤ºä¾‹:")
        for i, row in df.head(3).iterrows():
            print(f"   {i+1}. {row['title'][:80]}...")
            
    else:
        print("âŒ æœªè·å–åˆ°æ•°æ®")

if __name__ == "__main__":
    main()